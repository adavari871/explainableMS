{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning we have 3 datasets: train, validation and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# keras\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "\n",
    "from config import *\n",
    "from utils import specificity, sensitivity, balanced_accuracy, IntensityRescale, sagittal_flip, translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#percent = 0.5\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = percent\n",
    "config.gpu_options.visible_device_list = \"6\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_one_normalize = True\n",
    "dtype = np.float32\n",
    "result_dir = \"/analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load hdf5 files and extract columns\n",
    "train_h5 = h5py.File('/analysis/share/Ritter/MS/CIS/train_dataset_FLAIR_lesions_filled.h5', 'r')\n",
    "holdout_h5 = h5py.File('/analysis/share/Ritter/MS/CIS/holdout_dataset_FLAIR_lesions_filled.h5', 'r')\n",
    "\n",
    "X_train, y_train = train_h5['X'], train_h5['y']\n",
    "X_holdout, y_holdout = holdout_h5['X'], holdout_h5['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data to numpy arrays\n",
    "X_train = np.array(X_train, dtype=dtype)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_holdout = np.array(X_holdout, dtype=dtype)\n",
    "y_holdout = np.array(y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datset length: 124\n",
      "Number of healthy controls: 61\n",
      "Number of MS patients: 63\n"
     ]
    }
   ],
   "source": [
    "print(\"Total datset length: {}\".format(len(y_train)))\n",
    "print(\"Number of healthy controls: {}\".format(len(np.array(y_train)[np.array(y_train)==0.])))\n",
    "print(\"Number of MS patients: {}\".format(len(np.array(y_train)[np.array(y_train)==1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CISDataset(Sequence):\n",
    "    def __init__(self, X, y, transform=None, batch_size=4, z_factor=None, shuffle=True, mask=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.z_factor = z_factor\n",
    "        self.shuffle = shuffle\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # add BET\n",
    "        image = np.expand_dims(self.X[idx * self.batch_size:(idx + 1) * self.batch_size],5)\n",
    "        #label = np.array((batch_idx['label'] == \"MS\")* 1, dtype=np.int8) \n",
    "        label = np.array(self.y[idx * self.batch_size:(idx + 1) * self.batch_size], dtype=np.int8)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            for i in range(image.shape[0]):\n",
    "                image[i] *= self.mask\n",
    "        \n",
    "        for transformation in self.transform:\n",
    "            image = transformation(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.X, self.y = shuffle(self.X, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intensity = IntensityRescale(masked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if zero_one_normalize:\n",
    "    cis_data = CISDataset(X_train, y_train, transform=[intensity], batch_size=4)\n",
    "else:\n",
    "    cis_data = CISDataset(X_train, y_train, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Normalized between zero and 1\")\n",
    "plt.hist(cis_data[4][0][0].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Scan image\")\n",
    "plt.imshow(np.squeeze(cis_data[4][0][0])[:,:,42], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model(path, finetune=True, up_to=7):\n",
    "    model = load_model(path)\n",
    "    model.load_weights(path)\n",
    "    if finetune:\n",
    "        for layer in model.layers[:up_to]:\n",
    "            layer.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv3D)              (None, 94, 112, 94, 64)   1792      \n",
      "_________________________________________________________________\n",
      "Pool_1 (MaxPooling3D)        (None, 31, 37, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 31, 37, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv3D)              (None, 29, 35, 29, 64)    110656    \n",
      "_________________________________________________________________\n",
      "Pool_2 (MaxPooling3D)        (None, 9, 11, 9, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 9, 11, 9, 64)      0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv3D)              (None, 7, 9, 7, 64)       110656    \n",
      "_________________________________________________________________\n",
      "Conv_4 (Conv3D)              (None, 5, 7, 5, 64)       110656    \n",
      "_________________________________________________________________\n",
      "Pool_4 (MaxPooling3D)        (None, 1, 2, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1, 2, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 333,889\n",
      "Trainable params: 333,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load model weights\n",
    "#model_path = \"/analysis/share/Ritter/models/fabi/ADNI/pretraining_paper/model.h5\"\n",
    "model_path = \"/analysis/share/Ritter/models/fabi/ADNI/pretraining_paper/model_trial_7.h5\"\n",
    "model = init_model(model_path, finetune=False, up_to=None)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_weights(model):\n",
    "    # Visualize weights\n",
    "    W = model.layers[0].get_weights()[0]\n",
    "    W = np.squeeze(W)[:,:,2]\n",
    "    print(\"W shape : \", W.shape)\n",
    "\n",
    "    print(\"Weights mean {}\".format(W.mean()))\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.title('conv1 weights')\n",
    "    plt.imshow(make_mosaic(W, 2, 2), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "def make_mosaic(imgs, nrows, ncols, border=1):\n",
    "    \"\"\"\n",
    "    Given a set of images with all the same shape, makes a\n",
    "    mosaic with nrows and ncols\n",
    "    \"\"\"\n",
    "    nimgs = imgs.shape[0]\n",
    "    imshape = imgs.shape[1:]\n",
    "    \n",
    "    mosaic = ma.masked_all((nrows * imshape[0] + (nrows - 1) * border,\n",
    "                            ncols * imshape[1] + (ncols - 1) * border),\n",
    "                            dtype=np.float32)\n",
    "    \n",
    "    paddedh = imshape[0] + border\n",
    "    paddedw = imshape[1] + border\n",
    "    for i in range(nimgs):\n",
    "        row = int(np.floor(i / ncols))\n",
    "        col = i % ncols\n",
    "        \n",
    "        mosaic[row * paddedh:row * paddedh + imshape[0],\n",
    "               col * paddedw:col * paddedw + imshape[1]] = imgs[i]\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model layer 1 weights:\n",
      "W shape :  (3, 3, 64)\n",
      "Weights mean -0.0022606917191296816\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAABrCAYAAAAGj1lyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHOFJREFUeJzt3XmYVPX15/HPkU1kX0T2LRJQURYZFPklLhBFx2ASN4iJhvhTE2OM2ySiMybO6Dzx0eQ3OhKNUZNI1Lih4oKRkbBEEQXUyG6nAWlAQBFQUYR45o+6/NI23d9zCQ3VnXq/nqcfquocTn2r+lvfunX63lvm7gIAAAAAAEDp2K/YAwAAAAAAAMC+RUMIAAAAAACgxNAQAgAAAAAAKDE0hAAAAAAAAEoMDSEAAAAAAIASQ0MIAAAAAACgxNAQAgAA2IvM7E4z+x85c39nZjfs7TEBAADQEAIAACXDzDqZ2WQzW2NmbmY99/Z9uvv33P1/1UatbMwH10YtAABQ2mgIAQCAUvKZpOcknV7sgQAAABQTDSEAAFA0ZtbNzCaZ2QYze8/Mbs9u38/M/ruZrTSz9WZ2n5m1ymI9sz1lzjOzt83sXTO7Not1NrOPzaxtpfsYlOU0cvd17v4rSa/mGNs4M3uq0vUyM3u40vVVZjYwu9zPzKaa2UYzW2pmZ1XK+9xhYGb2YzNbm+2l9O/V7PXTxsyeMbMPzGyOmX0h+38zs/gbZvahmZ1tZu3N7Gkz25Td9ywzY/sOAACE2GAAAABFYWYNJD0taaWknpK6SPpjFv5O9nO8pN6Smku6vUqJf5PUV9IISdeZ2SHuvkbSbH1+D6BvSnrU3bfv5hBnSPpS1pzqJKmRpOHZ2HeO6a9m1kzSVEkPSOogaaykX5nZYdU85lGSrpA0UtLBko6t5n7HSrpeUhtJZZJulCR3/3IWH+Duzd39IUlXSqqQdKCkgyRdI8l383ECAIASREMIAAAUy1BJnSX9N3f/yN0/cfe/ZLFzJP3S3cvd/UNJ4yWNMbOGlf7/9e7+sbu/IekNSQOy2x9QoakiMzNJY7Lbdou7l0v6QNJAFRo3f5K02sz6Zddnuftnkk6VtMLdf+vuO9x9vqTHJJ1RTdmzJP3W3Re6+1YVGj9VTXL3V9x9h6T7s/uvyXZJnST1cPft7j7L3WkIAQCAEA0hAABQLN0krcwaH1V1VmHPoZ1WSmqowl4wO71T6fJWFfbYkaRHJQ0zs86SvqzCHjOz/skxzpB0XFZnhqTpKjSDjs2uS1IPSUdlh21tMrNNKjS0OtbwuFZVur6qmpyaHld1blZhL6LnzazczK6OHhAAAIBU2LACAAAohlWSuptZw2qaQmtUaLTs1F3SDknrJHVNFXX3TWb2vAp74xwi6cE92GtmhqSvSuol6X9L2tnsGaZ/HMK2StIMd/9Kjnpr9fnxd/snxyVJcvcPVDhs7MrsELU/m9mr7v7CntQFAAD/+thDCAAAFMsrKjRIfm5mzcxsfzMbnsUelHS5mfUys+YqNGMeqmFvouo8IOlcFc4l9LnDxcxsf0lNsqtNsus1maHCeYyaunuFCnsajZLUTtJrWc7Tkr5oZt82s0bZz38xs0OqqfewpHFmdoiZHSDpupyPZ6d1KpxTaedjOdXMDs4Ojdsi6e/ZDwAAQBINIQAAUBTu/ncV9r45WNLbKpwc+ewsfK+kiZJmSlou6RNJP9yN8pMl9ZG0LjvHUGUfS/owu7wku17TGJdlubOy61sklUt6MRv/zr10TlThXEVrVDjk6yb9o+lUud4USbdJ+rMKh3rNzkLbcj6un0n6fXZo2lnZY/x/2RhnS/qVu0/PWQsAAJQw47yDAAAAxZHtRbRAUpPd2PsJAABgj7GHEAAAwD5kZl83s8Zm1kaFPYmeohkEAAD2NRpCAAAA+9ZFkjZI+psK5/v5fnGHAwAAShGHjAEAAAAAAJQY9hACAAAAAAAoMQ2LdcctWrTw9u3bJ3M+/rjGL/2QJG3cuDG8n06dOoU527alv9hjv/3ivlnh217TGjZMP91NmuzyZSS7iPbo2rJlS1ijcePGYc4BBxyQjOd57ps1axbm5BlvJM9ebg0aNNjjGq1atQpzouclz+Nt27ZtmBM9t3//e/yNw5999lmYE82VPK+NPI+5Xbt2YU4kGsvbb78d1jjwwAPDnI8++igZz/M6zrNeVFRUJON5Xl957qdDhw7JeJ7Xeps2bcKcHTvSp0fJM+/XrVsX5kSPuVGjRmGN9957L8w56KCDkvE8a0o01jy/v02bNoU5LVq0SMbzrAV5nrcPP/wwGY/eVyTp008/DXNqY/3Lcz/RayPP2rb//qlvsy9YtmxZMh69f0n51uJoHuTZNvjggw/CnNatWyfjeX4/0Tqap8bmzZvDnGhe53lOom1VKf79RK8dKX5eJen9999Pxps2bRrWyLN2RetfntfG9u3bw5ytW7cm4y1btgxrRO/ZUjyf8jxv0Ws9z3t2nve46HnLs87mee6jz0Z51qU821SffPJJMp5nPuZZl6JtjDxrSjQfJaljx47J+Pr168Maed43ovf+PO/reeZKtB0SvU9K0sqVK/e4xpo1a8Kc6HNanvU8z/MWrdfRHJDieZ8nJ8/njbfffvtddw9fiEVrCLVv317XX399MmfBggXJ+P333x/ez09+8pMwp7y8PBnP84LJM8mihbFnz55hjWgjdtq0aWGNHj16hDmHH354Mv7II4+ENY466qgwZ8qUKWFOJPqgKcWLRJ43glNOOSXM+cMf/pCMT506NawxevToMOfoo49OxqONQinfRlI0J/NsJD333HNhzrhx45LxPB+Mo7F8//vxKTp+8IMfhDlz5sxJxnv37h3WyLNeXHHFFcl49BrNez8//GH6W7zzrLNnnnlmmBM1lsaOHRvWuOWWW8Kc6A0y+iAjSffdd1+Yc+WVVybjedal2mhePfnkk2HOiBEjkvE8G7l5Nthmz56djB955JFhjeXLl4c50fqX58PoqlWrwpxLLrkkGX/++efDGl/84hfDnK985SvJeJ4/RuRZi0844YRkPM82SJ5tjOg9LM+Ht169eu1xjcmTJ4c50R8Nu3fvHtZ48803w5yocTFjxoywxumnnx7mPPTQQ8l4//79wxp51q7LL788Gc+zrZOn+fHaa68l4yNHjgxrvPrqq2FO9P6U53nr169fMj506NCwxm233RbmvPPOO8n4gAEDwhp5nvu33norGc/TjLvooovCnKghnucD+vTp08OcMWPGJON5/vj1+uuvhzk//vGPk/EJEyaENfr27RvmRO/9ed7XBw4cGOZEOzNE25CSdPHFFyfjebbPf/azn4U5p556ajLetWvXsEaeRs2sWbOS8WuuuSassWjRojBn6dKlyXifPn3CGhdddFG6G5fJdciYmY0ys6VmVmZmV1cTb2JmD2XxOWbWM09dAAAAAAAA7HthQ8jMGkiaIOlkSYdKGmtmh1ZJO1/S++5+sKT/UOErVAEAAAAAAFAH5dlDaKikMncvd/dPJf1R0mlVck6T9Pvs8qOSRlieYzwAAAAAAACwz+VpCHWRVPlg+4rstmpz3H2HpM2SdjlLrJldaGZzzWxunuO/AQAAAAAAUPvyNISq29On6inf8+TI3e9y9yHuPiT6tgsAAAAAAADsHXkaQhWSulW63lVS1e9++88cM2soqZWk+HTtAAAAAAAA2OfyNIReldTHzHqZWWNJYyRV/S7PyZLOyy6fIWmau++yhxAAAAAAAACKr2GU4O47zOwSSX+S1EDSve6+0Mz+p6S57j5Z0j2SJppZmQp7Bo2J6jZq1EidO3dO5rz44ovJeOPGjaO7UbNmzcKc6HxGH374YVijbdu2YU67drucVulz3njjjbBGnrFEVqxYEeb85S9/ScbPPvvssEZFRUWYs99+6Z5knnOTjxo1KswZOXJkMj537tywRsuWLcOcq6++OhmP5rQknXvuuWHOQQcdlIxPmTIlrPHSSy+FOdHv8Gtf+1pY45hjjglzHnvssWR8yJAhYY2jjz46GT/zzDPDGu3btw9zDjvssGR8wIABYY08v5+JEycm4w0bhsu3nn/++TCnQ4cOyfg555wT1ti+fXuYM3PmzGR8zZqqO57uav78+WFONCfLy8vDGmeddVaYc++99ybjJ554Yljjs88+S8YffPDBsMbgwYPDnGher1q1KhmXpLKysjBn2LBhyXieQ8VbtWoV5kyfPj0Z33///cMazZs3D3MiixYtCnNefvnlMCd6D+vbt29YY9OmTWHOunXrkvF77rknrDF69Ogwp2PHjsl4tC0kSVOnTk3GBw0aFNa48847w5wzzjgjzInkWYt//etfJ+NXXHFFWKNNmzZhzlFHHZWM55mzebZBnn322WS8f//+YY08mjRpkoznWZe+/e1vhznXXXdd7jHVpGnTpsn4kiVLwhrRGipJM2bMSMZXrlwZ1jj55JPDnGuvvTYZHz9+fFjjF7/4RZgzduzYZPxb3/pWWOP4448Pcy6++OJkPPqcIEknnXRSmPP4448n49E2lyRNmzYtzInW80svvTSssWDBgjBnw4YNyXjr1q3DGtHn9Tyf1fO8D0bbD3k+/+bZfvjpT3+ajN94441hjZtvvjnM6dKl6imbP+/II48Ma1x00UVhjpSjISRJ7v6spGer3HZdpcufSIo/bQEAAAAAAKDo8hwyBgAAAAAAgH8hNIQAAAAAAABKDA0hAAAAAACAEkNDCAAAAAAAoMTQEAIAAAAAACgxNIQAAAAAAABKTK6vnd8b3F3bt29P5nzjG99Ixs0svJ+1a9eGOYMHD07G586dG9Zo2rRpmLN8+fJk/IADDghrvPfee8n4wIEDwxo7duwIc8rLy5Px2bNnhzWi51WSTjrppGT8ueeeC2t07do1zLnzzjuT8c6dO4c18ojmynHHHRfWePjhh8OcTz/9NBk///zzwxrTpk0LcwYMGJCM53kNvvXWW2FOjx49kvForZCkWbNmJeN9+vQJa2zYsCHMefnll5PxPHO2cePGYU7//v2T8Zdeeims0bBhvMSvWLEiGZ8wYUJY46tf/WqYc8EFFyTjTz/9dFije/fuYc7q1auT8S5duoQ17r777jBn2LBhyXiesa5bty4ZP/jgg8Mabdq0CXNeeeWVZDx6ziTplFNOCXPmzJmTjG/ZsiWscd5554U5w4cPT8afeuqpsEa0tuXxt7/9LcwpKysLc2699dZk/MEHHwxrfOELXwhzonW0Xbt2YY0GDRqEOdG689hjj4U1mjdvnowvXbo0rPHmm2+GOa1bt07G87yO33///TCnffv2yfgZZ5wR1sjzvB1zzDHJ+PTp08Ma0XupJL3zzjvJ+LJly8Iaed4Hr7rqqmR83rx5YY08r9NoLIMGDQprRI85z/bSkUceGeZE78k33XRTWCPPGhlt61x++eVhjUmTJoU50eeNn//852GNoUOHhjkPPPBAMn7zzTeHNRYvXhzm9OvXLxnPs54vWbIkzPnud7+bjOfZ5s2z5v/pT39Kxrdt2xbWiOZKnjUnz+84er+N1ntJOuKII8KciRMnJuN51raFCxeGOdE2Yp61LS/2EAIAAAAAACgxNIQAAAAAAABKDA0hAAAAAACAEkNDCAAAAAAAoMTQEAIAAAAAACgxYUPIzLqZ2Z/NbLGZLTSzH1WTc5yZbTaz17Of6/bOcAEAAAAAALCn8nzt/A5JV7r7fDNrIWmemU1190VV8ma5+6m1P0QAAAAAAADUpnAPIXdf6+7zs8sfSFosqcveHhgAAAAAAAD2DnP3/MlmPSXNlNTf3bdUuv04SY9JqpC0RtJV7r6wmv9/oaQLJal79+5Hrly5cg+GDgAAAAAAgMrMbJ67D4nycp9U2syaq9D0uaxyMygzX1IPdx8g6f9KeqK6Gu5+l7sPcfchBx54YN67BgAAAAAAQC3K1RAys0YqNIPud/dJVePuvsXdP8wuPyupkZm1r9WRAgAAAAAAoFbk+ZYxk3SPpMXu/ssacjpmeTKzoVnd92pzoAAAAAAAAKgdeb5lbLikb0t608xez267RlJ3SXL3OyWdIen7ZrZD0seSxvjunJwIAAAAAAAA+0zYEHL3v0iyIOd2SbfX1qAAAAAAAACw9+Q+qTQAAAAAAAD+NdAQAgAAAAAAKDE0hAAAAAAAAEpMnpNK7xUrVqzQuHHjkjm9e/dOxsvLy8P76dixY5gzfPjwZHzmzJlhjcaNG4c5xx57bDL+wgsvhDVWr16djF955ZVhjaVLl4Y52ZfG1WjEiBFhjfnz54c58+bNS8ZHjRoV1rjgggvCnPHjxyfjGzduDGt069YtzHnggQeS8dNPPz2sMWTIkDDnsssuS8ZvuOGGsMaCBQvCnOg1dtVVV4U1fvOb34Q5y5cvT8bvu+++sMYtt9yyR/chSc2bNw9z2rVrl4xPnz49rDF06NAwJ3qdtmzZMqyxaNGiMOeRRx5JxmfPnh3WiOa9JL377rvJ+CmnnBLWmDJlSphz4okn7tE4JGnw4MFhzsKFC5Pxb37zm2GNCRMmJON55mOetWvYsGHJ+KBBg8Iaedb8/v37J+ObN28Oa+R57svKypLxPGvoqlWrwpxbb701zAEAAKjP2EMIAAAAAACgxNAQAgAAAAAAKDE0hAAAAAAAAEoMDSEAAAAAAIASQ0MIAAAAAACgxNAQAgAAAAAAKDE0hAAAAAAAAEoMDSEAAAAAAIAS07BYd9ykSRP16dMnmbNt27ZkvGnTpuH9fOlLXwpzGjZMPw39+/cPa7z22mthTqNGjZLxwYMHhzWaNGmSjFdUVIQ1evXqFea89NJLyfijjz4a1mjQoEGY07Jly2Q8eryS1LVr1zCnQ4cOyXg0BySpU6dOYc4FF1yQjDdv3jys8Z3vfCfMueKKK5LxG264IawRPfeStGnTpmR848aNYY3oOZGkW2+9NRnP83guvfTSZDzP63jx4sVhTo8ePZLxqVOnhjWeeeaZMGf79u3J+CGHHBLWmDdvXphzySWX7PH9RM+JJB1//PHJ+BNPPBHWGD9+fJhzxx13JONlZWVhjZ49e4Y5hx9+eDI+Y8aMsMbHH3+cjK9fvz6s0a9fvzDno48+Ssaj17kknXvuuWHO8uXLk/GxY8eGNaZPnx7mRO+VCxYsCGuMGjUqzAEAAPhXl2sPITNbYWZvmtnrZja3mriZ2W1mVmZmfzWzuLMBAAAAAACAotidPYSOd/d3a4idLKlP9nOUpDuyfwEAAAAAAFDH1NY5hE6TdJ8XvCyptZnFx9cAAAAAAABgn8vbEHJJz5vZPDO7sJp4F0mrKl2vyG77HDO70Mzmmtnc6JwGAAAAAAAA2DvyHjI23N3XmFkHSVPNbIm7z6wUt2r+j+9yg/tdku6SpC5duuwSBwAAAAAAwN6Xaw8hd1+T/bte0uOShlZJqZDUrdL1rpLW1MYAAQAAAAAAULvChpCZNTOzFjsvSzpRUtXvdJ0s6dzs28aOlrTZ3dfW+mgBAAAAAACwx8w9feSWmfVWYa8gqXCI2QPufqOZfU+S3P1OMzNJt0saJWmrpHHuvsvX01fWqVMnHzduXPK+W7VqlYw3btw4GZek3r17hzlPPvlkMt63b9+wxrZt28KcgQMHJuMVFRVhje3bt+9RXJKWLFkS5jRr1iwZHz16dFhj1qxZYU6LFi2S8Txjbdu2bZgzYMCAZHzBgqo9zl316dMnzHnrrbeS8REjRoQ15s+fH+Y888wzyXjLli3DGp07dw5zojpt2rQJa9TG6yfPa3358uXJ+KpVq5JxSTriiCPCnK1btybjDz/8cFjjwgurOxXb7nnxxRfDnGXLloU5l112WTI+b968sEae33GDBg2S8TVr4p1Kzz777DBnypQpyfjatfHfKvI8ntmzZyfjhx56aFgjWv/yvK8MGTIkzJk0aVIyPnLkyLDG6tWrw5zo/XbixIlhjfLy8jDn3Xdr+sLTgq9//ethjdatW4c5559/fpgDAABQF5nZPHcPNxTDcwi5e7mkXT5Nu/udlS67pB/s7iABAAAAAACw79XW184DAAAAAACgnqAhBAAAAAAAUGJoCAEAAAAAAJQYGkIAAAAAAAAlhoYQAAAAAABAiaEhBAAAAAAAUGJoCAEAAAAAAJQYc/fi3LHZBkkrq9zcXtK7RRgO8M9izqK+Yc6ivmHOor5hzqK+Yc6ivmHOxnq4+4FRUtEaQtUxs7nuPqTY4wDyYs6ivmHOor5hzqK+Yc6ivmHOor5hztYeDhkDAAAAAAAoMTSEAAAAAAAASkxdawjdVewBALuJOYv6hjmL+oY5i/qGOYv6hjmL+oY5W0vq1DmEAAAAAAAAsPfVtT2EAAAAAAAAsJfREAIAAAAAACgxdaIhZGajzGypmZWZ2dXFHg9QlZl1M7M/m9liM1toZj/Kbm9rZlPN7K3s3zbFHitQmZk1MLPXzOzp7HovM5uTzdmHzKxxsccI7GRmrc3sUTNbkq23w1hnUZeZ2eXZdsECM3vQzPZnnUVdYmb3mtl6M1tQ6bZq11UruC37TPZXMxtcvJGjVNUwZ2/Otg3+amaPm1nrSrHx2ZxdamYnFWfU9VfRG0Jm1kDSBEknSzpU0lgzO7S4owJ2sUPSle5+iKSjJf0gm6dXS3rB3ftIeiG7DtQlP5K0uNL1myT9RzZn35d0flFGBVTvVknPuXs/SQNUmLuss6iTzKyLpEslDXH3/pIaSBoj1lnULb+TNKrKbTWtqydL6pP9XCjpjn00RqCy32nXOTtVUn93P0LSMknjJSn7PDZG0mHZ//lV1l9ATkVvCEkaKqnM3cvd/VNJf5R0WpHHBHyOu6919/nZ5Q9U+JDSRYW5+vss7feSvlacEQK7MrOukv6rpLuz6ybpBEmPZinMWdQZZtZS0pcl3SNJ7v6pu28S6yzqtoaSmppZQ0kHSFor1lnUIe4+U9LGKjfXtK6eJuk+L3hZUmsz67RvRgoUVDdn3f15d9+RXX1ZUtfs8mmS/uju29x9uaQyFfoLyKkuNIS6SFpV6XpFdhtQJ5lZT0mDJM2RdJC7r5UKTSNJHYo3MmAX/0fSjyV9ll1vJ2lTpTdU1lvUJb0lbZD02+wwx7vNrJlYZ1FHuftqSbdIeluFRtBmSfPEOou6r6Z1lc9lqA++K2lKdpk5u4fqQkPIqrnN9/kogBzMrLmkxyRd5u5bij0eoCZmdqqk9e4+r/LN1aSy3qKuaChpsKQ73H2QpI/E4WGow7LzrpwmqZekzpKaqXDITVWss6gv2E5AnWZm16pwKo/7d95UTRpzdjfUhYZQhaRula53lbSmSGMBamRmjVRoBt3v7pOym9ft3JU2+3d9scYHVDFc0mgzW6HCobgnqLDHUOvs0AaJ9RZ1S4WkCnefk11/VIUGEess6qqRkpa7+wZ33y5pkqRjxDqLuq+mdZXPZaizzOw8SadKOsfddzZ9mLN7qC40hF6V1Cf7RobGKpwUanKRxwR8TnbulXskLXb3X1YKTZZ0Xnb5PElP7uuxAdVx9/Hu3tXde6qwrk5z93Mk/VnSGVkacxZ1hru/I2mVmfXNbhohaZFYZ1F3vS3paDM7INtO2DlnWWdR19W0rk6WdG72bWNHS9q889AyoJjMbJSkn0ga7e5bK4UmSxpjZk3MrJcKJ0R/pRhjrK/sH821Ig7C7BQV/nLdQNK97n5jkYcEfI6Z/ZukWZLe1D/Ox3KNCucRelhSdxU2DM9096on7gOKysyOk3SVu59qZr1V2GOoraTXJH3L3bcVc3zATmY2UIWToDeWVC5pnAp/vGKdRZ1kZtdLOluFQxhek/TvKpy/gnUWdYKZPSjpOEntJa2T9FNJT6iadTVrbN6uwrc1bZU0zt3nFmPcKF01zNnxkppIei9Le9ndv5flX6vCeYV2qHBajylVa6JmdaIhBAAAAAAAgH2nLhwyBgAAAAAAgH2IhhAAAAAAAECJoSEEAAAAAABQYmgIAQAAAAAAlBgaQgAAAAAAACWGhhAAAAAAAECJoSEEAAAAAABQYv4/dS55YB1uDSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random model layer 1 weights:\n",
      "W shape :  (3, 3, 64)\n",
      "Weights mean 0.0009204132948070765\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize weights\n",
    "print(\"Pre-trained model layer 1 weights:\")\n",
    "visualize_weights(model)\n",
    "model_untrained = load_model(model_path)\n",
    "reset_weights(model_untrained)\n",
    "print(\"Random model layer 1 weights:\")\n",
    "visualize_weights(model_untrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 96, 114, 96)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 9s 304ms/step - loss: 0.9307 - acc: 0.5416 - val_loss: 0.9173 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial0-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 161ms/step - loss: 0.9812 - acc: 0.5148 - val_loss: 0.7927 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30769 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial0-improvement-BEST.hdf5\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.9093 - acc: 0.5388 - val_loss: 1.0951 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.76923\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.9146 - acc: 0.5537 - val_loss: 0.8819 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76923\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.9001 - acc: 0.4494 - val_loss: 0.8703 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76923\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 176ms/step - loss: 0.8749 - acc: 0.4731 - val_loss: 0.9492 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76923\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.8207 - acc: 0.5448 - val_loss: 0.7971 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76923\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.7872 - acc: 0.6041 - val_loss: 0.8263 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76923\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7674 - acc: 0.6309 - val_loss: 0.8859 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76923\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 184ms/step - loss: 0.7838 - acc: 0.5506 - val_loss: 0.7219 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76923\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 176ms/step - loss: 0.7094 - acc: 0.7053 - val_loss: 0.8391 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76923\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.7471 - acc: 0.5655 - val_loss: 1.0132 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76923\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.7027 - acc: 0.6874 - val_loss: 0.7243 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76923\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 177ms/step - loss: 0.6206 - acc: 0.7738 - val_loss: 0.7250 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76923\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.6152 - acc: 0.7946 - val_loss: 0.6784 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76923\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.6674 - acc: 0.7560 - val_loss: 0.6089 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76923\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.5500 - acc: 0.8274 - val_loss: 0.5907 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76923\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.5485 - acc: 0.8482 - val_loss: 0.6420 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76923\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.5215 - acc: 0.8839 - val_loss: 0.6793 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76923\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.5316 - acc: 0.8006 - val_loss: 0.6375 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76923\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.4937 - acc: 0.8364 - val_loss: 0.6323 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76923\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.4572 - acc: 0.9107 - val_loss: 0.7496 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76923\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.4775 - acc: 0.8453 - val_loss: 0.6911 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76923\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.4290 - acc: 0.8750 - val_loss: 0.5648 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76923\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4420 - acc: 0.8839 - val_loss: 0.5904 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76923\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.4180 - acc: 0.9107 - val_loss: 0.6214 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76923\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.3915 - acc: 0.9107 - val_loss: 0.5346 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76923\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 5s 174ms/step - loss: 0.3794 - acc: 0.8810 - val_loss: 0.7796 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76923\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.4544 - acc: 0.8928 - val_loss: 0.7245 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76923\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.3881 - acc: 0.9375 - val_loss: 0.6270 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76923\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 5s 174ms/step - loss: 0.3719 - acc: 0.9196 - val_loss: 0.5849 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76923\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.4526 - acc: 0.8928 - val_loss: 0.5642 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76923\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 5s 178ms/step - loss: 0.3871 - acc: 0.9285 - val_loss: 0.5587 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76923\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.3996 - acc: 0.9196 - val_loss: 0.5954 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76923\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 4s 154ms/step - loss: 0.3438 - acc: 0.9375 - val_loss: 0.7825 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76923\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3384 - acc: 0.9196 - val_loss: 0.8086 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76923\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.3481 - acc: 0.9196 - val_loss: 0.6422 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76923\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.2913 - acc: 0.9464 - val_loss: 0.7981 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76923\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.3440 - acc: 0.9346 - val_loss: 0.6738 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76923\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.3117 - acc: 0.9375 - val_loss: 0.7305 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76923\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.3257 - acc: 0.9285 - val_loss: 0.9441 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76923\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 5s 174ms/step - loss: 0.3303 - acc: 0.9285 - val_loss: 0.6125 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76923\n",
      "Epoch 00042: early stopping\n",
      "Trial 1\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 1.0545 - acc: 0.4880 - val_loss: 1.1761 - val_acc: 0.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 153ms/step - loss: 0.9785 - acc: 0.4644 - val_loss: 1.2222 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.30769\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.9783 - acc: 0.3959 - val_loss: 0.8872 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.30769 to 0.46154, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.9123 - acc: 0.5506 - val_loss: 1.0904 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.46154\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8356 - acc: 0.5863 - val_loss: 0.7564 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.46154 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.8423 - acc: 0.5537 - val_loss: 1.0940 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.8349 - acc: 0.5595 - val_loss: 0.7181 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 181ms/step - loss: 0.8289 - acc: 0.6102 - val_loss: 0.8910 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7648 - acc: 0.6249 - val_loss: 0.8160 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.7724 - acc: 0.6013 - val_loss: 0.8081 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.7707 - acc: 0.6220 - val_loss: 0.6968 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.69231 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.6990 - acc: 0.6667 - val_loss: 0.6797 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76923\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.6728 - acc: 0.7470 - val_loss: 0.8664 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76923\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.6221 - acc: 0.7738 - val_loss: 0.6942 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76923\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.6385 - acc: 0.7321 - val_loss: 0.5920 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76923\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.5909 - acc: 0.8214 - val_loss: 0.8285 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76923\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 6s 202ms/step - loss: 0.5758 - acc: 0.7946 - val_loss: 0.6632 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76923\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.5422 - acc: 0.8274 - val_loss: 0.6975 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76923\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.5614 - acc: 0.8364 - val_loss: 0.7154 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76923\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.5104 - acc: 0.8571 - val_loss: 0.6997 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76923\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.4406 - acc: 0.9196 - val_loss: 0.5728 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76923\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.4472 - acc: 0.9018 - val_loss: 0.5819 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76923\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.4580 - acc: 0.9078 - val_loss: 0.5702 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76923\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4729 - acc: 0.8721 - val_loss: 0.5892 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.76923 to 0.84615, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.3972 - acc: 0.9018 - val_loss: 0.5920 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.84615\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.4401 - acc: 0.8900 - val_loss: 0.5995 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84615\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 187ms/step - loss: 0.4239 - acc: 0.8928 - val_loss: 0.6428 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84615\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.4150 - acc: 0.9107 - val_loss: 0.6209 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84615\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.4189 - acc: 0.8928 - val_loss: 0.7038 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84615\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.3544 - acc: 0.9375 - val_loss: 0.5976 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84615\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.3893 - acc: 0.9107 - val_loss: 0.6107 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84615\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.3758 - acc: 0.9196 - val_loss: 0.7731 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84615\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 4s 161ms/step - loss: 0.3729 - acc: 0.9285 - val_loss: 0.6200 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84615\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.3839 - acc: 0.9375 - val_loss: 0.6470 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84615\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.3561 - acc: 0.9167 - val_loss: 0.6819 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84615\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 5s 176ms/step - loss: 0.3173 - acc: 0.9464 - val_loss: 0.9210 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84615\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.3355 - acc: 0.9346 - val_loss: 0.6502 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84615\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.3561 - acc: 0.9107 - val_loss: 0.5823 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84615\n",
      "Epoch 00038: early stopping\n",
      "Trial 2\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 218ms/step - loss: 1.0214 - acc: 0.5030 - val_loss: 0.9667 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial2-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 149ms/step - loss: 0.8977 - acc: 0.5595 - val_loss: 1.0150 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.30769\n",
      "Epoch 3/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 5s 165ms/step - loss: 0.9554 - acc: 0.4644 - val_loss: 0.8603 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.30769\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.8990 - acc: 0.4612 - val_loss: 0.8650 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.30769\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 180ms/step - loss: 0.8341 - acc: 0.5148 - val_loss: 0.7254 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.30769 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial2-improvement-BEST.hdf5\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.8539 - acc: 0.5088 - val_loss: 0.8638 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8432 - acc: 0.5624 - val_loss: 0.7013 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8645 - acc: 0.5002 - val_loss: 0.8615 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.8125 - acc: 0.5177 - val_loss: 0.8295 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.8000 - acc: 0.5863 - val_loss: 0.7690 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7610 - acc: 0.6488 - val_loss: 0.8155 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.7346 - acc: 0.6606 - val_loss: 0.8212 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7125 - acc: 0.6338 - val_loss: 0.8762 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.6752 - acc: 0.6963 - val_loss: 0.7360 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 6s 208ms/step - loss: 0.6014 - acc: 0.8482 - val_loss: 0.6391 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.69231 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial2-improvement-BEST.hdf5\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.6136 - acc: 0.7410 - val_loss: 0.6610 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76923\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.5027 - acc: 0.8721 - val_loss: 0.9745 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76923\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.6235 - acc: 0.7678 - val_loss: 0.7590 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76923\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.5454 - acc: 0.7917 - val_loss: 0.6208 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76923\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.5242 - acc: 0.8185 - val_loss: 0.6779 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76923\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.4731 - acc: 0.8989 - val_loss: 0.5508 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76923\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.4305 - acc: 0.8839 - val_loss: 0.5684 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76923\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.4740 - acc: 0.8750 - val_loss: 0.7662 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76923\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.4708 - acc: 0.8750 - val_loss: 0.6170 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76923\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.4240 - acc: 0.8900 - val_loss: 0.6557 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76923\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.4107 - acc: 0.9018 - val_loss: 0.7033 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76923\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4337 - acc: 0.8660 - val_loss: 0.5860 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76923\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 4s 161ms/step - loss: 0.3745 - acc: 0.9285 - val_loss: 0.7384 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76923\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.4012 - acc: 0.9018 - val_loss: 0.7571 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76923\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.3948 - acc: 0.9167 - val_loss: 0.6922 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76923\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.3728 - acc: 0.9196 - val_loss: 0.6627 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76923\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.3520 - acc: 0.9464 - val_loss: 0.6504 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76923\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.3461 - acc: 0.9346 - val_loss: 0.7073 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76923\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4024 - acc: 0.9285 - val_loss: 0.6755 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76923\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.3777 - acc: 0.9107 - val_loss: 0.6394 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76923\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.3929 - acc: 0.9375 - val_loss: 0.7343 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76923\n",
      "Epoch 00036: early stopping\n",
      "Trial 3\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 228ms/step - loss: 1.0477 - acc: 0.5209 - val_loss: 0.9552 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial3-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 153ms/step - loss: 1.0098 - acc: 0.4405 - val_loss: 1.0330 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.30769\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.9064 - acc: 0.6041 - val_loss: 0.9363 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.30769 to 0.46154, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial3-improvement-BEST.hdf5\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.9228 - acc: 0.5120 - val_loss: 0.9422 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.46154\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8564 - acc: 0.5448 - val_loss: 0.7837 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.46154 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial3-improvement-BEST.hdf5\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.8511 - acc: 0.6459 - val_loss: 0.8444 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76923\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 5s 161ms/step - loss: 0.8962 - acc: 0.5416 - val_loss: 0.7534 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76923\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.8512 - acc: 0.5148 - val_loss: 1.1151 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76923\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 4s 161ms/step - loss: 0.8351 - acc: 0.5269 - val_loss: 0.7804 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76923\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8134 - acc: 0.5774 - val_loss: 0.7778 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76923\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 179ms/step - loss: 0.7736 - acc: 0.6220 - val_loss: 0.8018 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76923\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.7588 - acc: 0.6131 - val_loss: 0.8793 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76923\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.7351 - acc: 0.6338 - val_loss: 0.6729 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76923\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.6908 - acc: 0.7499 - val_loss: 0.7117 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.76923 to 0.84615, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial3-improvement-BEST.hdf5\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.6693 - acc: 0.7231 - val_loss: 0.7277 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.84615\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.5804 - acc: 0.8185 - val_loss: 0.7090 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.84615\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.5579 - acc: 0.7917 - val_loss: 0.7295 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.84615\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.5489 - acc: 0.8364 - val_loss: 0.8180 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.84615\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.6005 - acc: 0.7856 - val_loss: 0.6309 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.84615\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.5410 - acc: 0.8571 - val_loss: 0.6142 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.84615\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.5039 - acc: 0.8750 - val_loss: 0.5924 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.84615\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.5253 - acc: 0.8335 - val_loss: 0.6620 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.84615\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.4340 - acc: 0.8750 - val_loss: 0.6279 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.84615\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.4658 - acc: 0.8750 - val_loss: 0.8273 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.84615\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4310 - acc: 0.9078 - val_loss: 0.6950 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.84615\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.4040 - acc: 0.9107 - val_loss: 0.6951 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.84615\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.3829 - acc: 0.9167 - val_loss: 0.7790 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.84615\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 5s 179ms/step - loss: 0.3820 - acc: 0.9285 - val_loss: 0.6036 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.84615\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 5s 180ms/step - loss: 0.3758 - acc: 0.9464 - val_loss: 0.6869 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.84615\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.3564 - acc: 0.9196 - val_loss: 0.9068 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.84615\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.3890 - acc: 0.9257 - val_loss: 0.7725 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.84615\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.3675 - acc: 0.9107 - val_loss: 0.7166 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.84615\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.3927 - acc: 0.8928 - val_loss: 0.6776 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.84615\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.3443 - acc: 0.9285 - val_loss: 0.7016 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84615\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.3405 - acc: 0.9375 - val_loss: 0.7105 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84615\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.3218 - acc: 0.9464 - val_loss: 0.7093 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84615\n",
      "Epoch 00036: early stopping\n",
      "Trial 4\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 224ms/step - loss: 1.0149 - acc: 0.5120 - val_loss: 0.8616 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial4-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 153ms/step - loss: 0.9850 - acc: 0.4791 - val_loss: 0.9551 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.9433 - acc: 0.5863 - val_loss: 0.8586 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.9386 - acc: 0.4912 - val_loss: 0.9500 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.8464 - acc: 0.5981 - val_loss: 0.9457 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8688 - acc: 0.6281 - val_loss: 0.8104 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.8180 - acc: 0.5416 - val_loss: 0.9198 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 180ms/step - loss: 0.8158 - acc: 0.5774 - val_loss: 0.8932 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.7654 - acc: 0.6695 - val_loss: 0.7887 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.7789 - acc: 0.5774 - val_loss: 0.9052 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.7110 - acc: 0.7321 - val_loss: 0.6509 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.7270 - acc: 0.6638 - val_loss: 0.8983 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.6865 - acc: 0.7084 - val_loss: 0.7680 - val_acc: 0.6923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.6309 - acc: 0.7470 - val_loss: 0.6618 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.5730 - acc: 0.8214 - val_loss: 0.7187 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.69231 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial4-improvement-BEST.hdf5\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 4s 154ms/step - loss: 0.5451 - acc: 0.8303 - val_loss: 0.5971 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76923\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.5534 - acc: 0.8392 - val_loss: 0.7589 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76923\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.5261 - acc: 0.8453 - val_loss: 0.6897 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76923\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.5265 - acc: 0.8274 - val_loss: 0.7162 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76923\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.4954 - acc: 0.8482 - val_loss: 0.6196 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76923\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.4320 - acc: 0.8750 - val_loss: 0.6302 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76923\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.5151 - acc: 0.8303 - val_loss: 0.7239 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76923\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.4697 - acc: 0.8839 - val_loss: 0.6405 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76923\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.4311 - acc: 0.8928 - val_loss: 0.7774 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76923\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.4176 - acc: 0.9257 - val_loss: 0.7868 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76923\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.3812 - acc: 0.9285 - val_loss: 0.6166 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76923\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.4044 - acc: 0.9018 - val_loss: 0.6966 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76923\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3982 - acc: 0.9196 - val_loss: 0.8246 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76923\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.3765 - acc: 0.9285 - val_loss: 0.6048 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76923\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 187ms/step - loss: 0.3903 - acc: 0.9196 - val_loss: 0.7940 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76923\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.3744 - acc: 0.9107 - val_loss: 0.5600 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76923\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.3489 - acc: 0.9464 - val_loss: 0.6939 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76923\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.3749 - acc: 0.9285 - val_loss: 0.5966 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00033: val_acc improved from 0.76923 to 0.84615, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial4-improvement-BEST.hdf5\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.3120 - acc: 0.9464 - val_loss: 0.7002 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.84615\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 5s 191ms/step - loss: 0.3333 - acc: 0.9464 - val_loss: 0.6838 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.84615\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 4s 155ms/step - loss: 0.3671 - acc: 0.9285 - val_loss: 0.6649 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.84615\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3313 - acc: 0.9375 - val_loss: 0.6898 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.84615\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.3303 - acc: 0.9553 - val_loss: 0.6147 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.84615\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3210 - acc: 0.9464 - val_loss: 0.8849 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84615\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.3514 - acc: 0.9285 - val_loss: 0.6601 - val_acc: 0.8462\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.84615\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.3043 - acc: 0.9375 - val_loss: 0.5840 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84615\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.3076 - acc: 0.9464 - val_loss: 0.6240 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84615\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.3481 - acc: 0.9107 - val_loss: 0.6602 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84615\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.3328 - acc: 0.9375 - val_loss: 0.6465 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84615\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.3218 - acc: 0.9285 - val_loss: 0.6894 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84615\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.3175 - acc: 0.9553 - val_loss: 0.6982 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84615\n",
      "Epoch 00046: early stopping\n",
      "Trial 5\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 228ms/step - loss: 1.0490 - acc: 0.4555 - val_loss: 0.8732 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial5-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 151ms/step - loss: 0.9216 - acc: 0.4584 - val_loss: 0.8586 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.9011 - acc: 0.4970 - val_loss: 0.8088 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.9238 - acc: 0.4644 - val_loss: 0.7446 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.9368 - acc: 0.5327 - val_loss: 0.8175 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.8813 - acc: 0.5180 - val_loss: 0.9380 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.7695 - acc: 0.6459 - val_loss: 0.8006 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8148 - acc: 0.4970 - val_loss: 0.7949 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.7869 - acc: 0.5327 - val_loss: 0.8152 - val_acc: 0.4615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.7249 - acc: 0.6577 - val_loss: 0.6800 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.69231 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial5-improvement-BEST.hdf5\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.7954 - acc: 0.6220 - val_loss: 0.7441 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76923\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.7284 - acc: 0.6399 - val_loss: 0.8036 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76923\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.7040 - acc: 0.6609 - val_loss: 0.8081 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76923\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.6867 - acc: 0.6935 - val_loss: 0.7145 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76923\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.6031 - acc: 0.8303 - val_loss: 0.7849 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76923\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 4s 161ms/step - loss: 0.5723 - acc: 0.8214 - val_loss: 0.9468 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76923\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.5996 - acc: 0.7767 - val_loss: 0.6548 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76923\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.5071 - acc: 0.8571 - val_loss: 0.6277 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76923\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 4s 154ms/step - loss: 0.5629 - acc: 0.7888 - val_loss: 0.7676 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76923\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.5235 - acc: 0.7856 - val_loss: 0.6562 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76923\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.4828 - acc: 0.8214 - val_loss: 0.6069 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76923\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.4629 - acc: 0.8632 - val_loss: 0.6273 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76923\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.4768 - acc: 0.8839 - val_loss: 0.7911 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76923\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.4291 - acc: 0.9018 - val_loss: 0.7053 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76923\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.4400 - acc: 0.8810 - val_loss: 0.6590 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76923\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.4331 - acc: 0.9107 - val_loss: 0.6210 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76923\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.4299 - acc: 0.8839 - val_loss: 0.6314 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76923\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.4272 - acc: 0.8871 - val_loss: 0.6588 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76923\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.3870 - acc: 0.9107 - val_loss: 0.6375 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76923\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.3910 - acc: 0.9196 - val_loss: 0.6333 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76923\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.3402 - acc: 0.9285 - val_loss: 0.6448 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.76923\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.3709 - acc: 0.8750 - val_loss: 0.6264 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.76923\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3396 - acc: 0.9285 - val_loss: 0.6745 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.76923\n",
      "Epoch 34/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.3648 - acc: 0.9196 - val_loss: 0.5935 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.76923\n",
      "Epoch 35/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3495 - acc: 0.9285 - val_loss: 0.6834 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.76923\n",
      "Epoch 36/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.3561 - acc: 0.9167 - val_loss: 0.5771 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.76923\n",
      "Epoch 37/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.3429 - acc: 0.9285 - val_loss: 0.6482 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.76923\n",
      "Epoch 38/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.3759 - acc: 0.9375 - val_loss: 0.5876 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.76923\n",
      "Epoch 39/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.3490 - acc: 0.9018 - val_loss: 0.6442 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.76923\n",
      "Epoch 40/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.3325 - acc: 0.9196 - val_loss: 0.6570 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.76923\n",
      "Epoch 41/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.2972 - acc: 0.9553 - val_loss: 0.6333 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.76923\n",
      "Epoch 42/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.2880 - acc: 0.9732 - val_loss: 0.6543 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.76923\n",
      "Epoch 43/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.3523 - acc: 0.9525 - val_loss: 0.6062 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.76923\n",
      "Epoch 44/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3026 - acc: 0.9375 - val_loss: 0.6545 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.76923\n",
      "Epoch 45/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.3091 - acc: 0.9375 - val_loss: 0.6436 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.76923\n",
      "Epoch 46/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.2890 - acc: 0.9375 - val_loss: 0.6342 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.76923\n",
      "Epoch 47/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.2839 - acc: 0.9375 - val_loss: 0.7497 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.76923\n",
      "Epoch 48/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.2752 - acc: 0.9435 - val_loss: 0.6161 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.76923\n",
      "Epoch 49/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.2592 - acc: 0.9643 - val_loss: 0.7286 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.76923\n",
      "Epoch 50/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.2797 - acc: 0.9375 - val_loss: 0.6762 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.76923\n",
      "Epoch 51/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.2623 - acc: 0.9464 - val_loss: 0.7320 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.76923\n",
      "Epoch 00051: early stopping\n",
      "Trial 6\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 7s 246ms/step - loss: 0.9884 - acc: 0.5209 - val_loss: 0.8877 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46154, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial6-improvement-BEST.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 153ms/step - loss: 0.9985 - acc: 0.4345 - val_loss: 0.9007 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.46154\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.8900 - acc: 0.5624 - val_loss: 0.7998 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.46154 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/remove_lesions/weights-augm-trial6-improvement-BEST.hdf5\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.9025 - acc: 0.5177 - val_loss: 0.7375 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76923\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 4s 155ms/step - loss: 0.8506 - acc: 0.5923 - val_loss: 0.8533 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.76923\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.8026 - acc: 0.5805 - val_loss: 0.8326 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.76923\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 177ms/step - loss: 0.7750 - acc: 0.6549 - val_loss: 0.7892 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.76923\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.6677 - acc: 0.7410 - val_loss: 0.9339 - val_acc: 0.3846\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.76923\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.7033 - acc: 0.7321 - val_loss: 0.6960 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.76923\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.7121 - acc: 0.7024 - val_loss: 0.7709 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76923\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.6588 - acc: 0.7589 - val_loss: 0.9869 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76923\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.6231 - acc: 0.7589 - val_loss: 0.6192 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76923\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.5988 - acc: 0.8246 - val_loss: 1.0060 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76923\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.5694 - acc: 0.8482 - val_loss: 0.8969 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76923\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.5723 - acc: 0.7946 - val_loss: 0.6025 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76923\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.5036 - acc: 0.8214 - val_loss: 0.6771 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76923\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.4651 - acc: 0.9167 - val_loss: 0.6067 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.76923\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.4822 - acc: 0.8721 - val_loss: 0.7316 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.76923\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.4725 - acc: 0.8632 - val_loss: 0.6256 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.76923\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4860 - acc: 0.8839 - val_loss: 0.7320 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.76923\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.4475 - acc: 0.8928 - val_loss: 0.6375 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.76923\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.4395 - acc: 0.8750 - val_loss: 0.6576 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.76923\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.4513 - acc: 0.8750 - val_loss: 0.7779 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.76923\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.4166 - acc: 0.9196 - val_loss: 0.6588 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.76923\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.4185 - acc: 0.8989 - val_loss: 0.7517 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.76923\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.4000 - acc: 0.9464 - val_loss: 0.6798 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.76923\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3830 - acc: 0.9464 - val_loss: 0.6809 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.76923\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.3435 - acc: 0.9375 - val_loss: 0.6707 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.76923\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.3282 - acc: 0.9614 - val_loss: 0.6993 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.76923\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.3633 - acc: 0.9257 - val_loss: 0.7926 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.76923\n",
      "Epoch 00030: early stopping\n",
      "Training Time: 0.0h:23.0m:54.640684843063354s\n",
      "Validation final accuracies: \n",
      " [0.6923076923076923, 0.7692307692307693, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923]\n",
      "Validation final accuracies mean: 0.7032967032967035\n",
      "Validation best accuracies: \n",
      " [0.7692307692307693, 0.8461538461538461, 0.7692307692307693, 0.8461538461538461, 0.8461538461538461, 0.7692307692307693, 0.7692307692307693]\n",
      "Validation best accuracies mean: 0.8021978021978021\n",
      "Validation balanced accuracies: \n",
      " [0.6388888888888888, 0.7638888888888888, 0.7083333333333333, 0.6388888888888888, 0.7083333333333333, 0.7083333333333333, 0.7777777777777778]\n",
      "Validation balanced accuracies mean: 0.7063492063492063\n",
      "Validation final sensitivities: \n",
      " [0.5, 0.75, 0.75, 0.5, 0.75, 0.75, 1.0]\n",
      "Validation final sensitivities' mean: 0.7142857142857143\n",
      "Validation final specificities: \n",
      " [0.7777777777777778, 0.7777777777777778, 0.6666666666666666, 0.7777777777777778, 0.6666666666666666, 0.6666666666666666, 0.5555555555555556]\n",
      "Validation final specificities' mean: 0.6984126984126984\n"
     ]
    }
   ],
   "source": [
    "# training args\n",
    "lr = 0.0008\n",
    "lr_decay = 0.002\n",
    "transforms = [intensity, sagittal_flip, translate]\n",
    "\n",
    "num_trials = 7\n",
    "store_models = True\n",
    "\n",
    "# callbacks\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "max_acc = []\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"Trial %i\" %i)\n",
    "    \n",
    "    # init model\n",
    "    model = init_model(model_path, finetune=False, up_to=None)    \n",
    "    opti = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # callbacks\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=15, verbose=1, mode='auto')\n",
    "    if store_models:\n",
    "        result_path = os.path.join(result_dir, \"weights-augm-trial%i-improvement-BEST.hdf5\" %i)\n",
    "        model_checkpoint = ModelCheckpoint(result_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "        callbacks = [earlystop, model_checkpoint]\n",
    "    else:\n",
    "        callbacks = [earlystop]\n",
    "        \n",
    "    train_loader = CISDataset(X_train, y_train, transform=transforms, batch_size=b, shuffle=True)\n",
    "    val_loader = CISDataset(X_val, y_val, transform=[intensity], batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Start training\n",
    "    history = model.fit_generator(train_loader,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=val_loader,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict_generator(val_loader)\n",
    "    #y_true = [item for sublist in [val_loader[batch_idx][1] for batch_idx in range(len(val_loader))] for item in sublist]\n",
    "    bal_acc = balanced_accuracy(y_val, y_pred>0.5)\n",
    "    sens = sensitivity(y_val, y_pred>0.5)\n",
    "    spec = specificity(y_val, y_pred>0.5)\n",
    "    # Store results\n",
    "    accuracies.append(history.history[\"val_acc\"][-1])\n",
    "    balanced_accuracies.append(bal_acc)\n",
    "    max_acc.append(np.max(history.history[\"val_acc\"]))\n",
    "    sensitivities.append(sens)\n",
    "    specificities.append(spec)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training Time: {}h:{}m:{}s\".format(\n",
    "            training_time//3600, (training_time//60)%60, training_time%60))\n",
    "\n",
    "print(\"Validation final accuracies: \\n {}\".format(accuracies))\n",
    "print(\"Validation final accuracies mean: {}\".format(np.mean(accuracies)))\n",
    "print(\"Validation best accuracies: \\n {}\".format(max_acc))\n",
    "print(\"Validation best accuracies mean: {}\".format(np.mean(max_acc)))\n",
    "print(\"Validation balanced accuracies: \\n {}\".format(balanced_accuracies))\n",
    "print(\"Validation balanced accuracies mean: {}\".format(np.mean(balanced_accuracies)))\n",
    "print(\"Validation final sensitivities: \\n {}\".format(sensitivities))\n",
    "print(\"Validation final sensitivities' mean: {}\".format(np.mean(sensitivities)))\n",
    "print(\"Validation final specificities: \\n {}\".format(specificities))\n",
    "print(\"Validation final specificities' mean: {}\".format(np.mean(specificities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [\"weights-augm-trial%i-improvement-BEST.hdf5\"%i for i in range(num_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load holdout set\n",
    "test_loader = CISDataset(X_holdout, y_holdout, transform=[intensity], batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy 56.52 %\n",
      "Balanced accuracy 56.92 %\n",
      "Sensitivity 53.85 %\n",
      "Specificity 60.00 %\n",
      "Fold 1\n",
      "Model accuracy 86.96 %\n",
      "Balanced accuracy 87.31 %\n",
      "Sensitivity 84.62 %\n",
      "Specificity 90.00 %\n",
      "Fold 2\n",
      "Model accuracy 78.26 %\n",
      "Balanced accuracy 77.31 %\n",
      "Sensitivity 84.62 %\n",
      "Specificity 70.00 %\n",
      "Fold 3\n",
      "Model accuracy 69.57 %\n",
      "Balanced accuracy 67.31 %\n",
      "Sensitivity 84.62 %\n",
      "Specificity 50.00 %\n",
      "Fold 4\n",
      "Model accuracy 82.61 %\n",
      "Balanced accuracy 82.31 %\n",
      "Sensitivity 84.62 %\n",
      "Specificity 80.00 %\n",
      "Fold 5\n",
      "Model accuracy 47.83 %\n",
      "Balanced accuracy 52.69 %\n",
      "Sensitivity 15.38 %\n",
      "Specificity 90.00 %\n",
      "Fold 6\n",
      "Model accuracy 52.17 %\n",
      "Balanced accuracy 55.38 %\n",
      "Sensitivity 30.77 %\n",
      "Specificity 80.00 %\n",
      "######## Final results ########\n",
      "Accuracy mean 67.70 %\n",
      "Balanced accuracy mean 68.46 %\n",
      "Sensitivity mean 62.64 %\n",
      "Specificity mean 74.29 %\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "for fold, weight in enumerate(weights):\n",
    "    print(\"Fold {}\".format(fold))\n",
    "    model = load_model(model_path)\n",
    "    model_dir = os.path.join(result_dir, weight)\n",
    "    model.load_weights(model_dir)\n",
    "    \n",
    "    opti = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(optimizer=opti,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Evaluate\n",
    "    res = model.evaluate_generator(test_loader)\n",
    "    y_pred = model.predict_generator(test_loader)\n",
    "    bal_acc = balanced_accuracy(y_holdout, y_pred>0.5)\n",
    "    sens = sensitivity(y_holdout, y_pred>0.5)\n",
    "    spec = specificity(y_holdout, y_pred>0.5)\n",
    "    # Store results\n",
    "    accuracies.append(res[1])\n",
    "    balanced_accuracies.append(bal_acc)\n",
    "    sensitivities.append(sens)\n",
    "    specificities.append(spec)\n",
    "    # Print results\n",
    "    print(\"Model accuracy {:.2f} %\".format(res[1]*100))\n",
    "    print(\"Balanced accuracy {:.2f} %\".format(bal_acc*100))\n",
    "    print(\"Sensitivity {:.2f} %\".format(sens*100))\n",
    "    print(\"Specificity {:.2f} %\".format(spec*100))\n",
    "    \n",
    "    \n",
    "print(\"######## Final results ########\")\n",
    "print(\"Accuracy mean {:.2f} %\".format(np.mean(accuracies)*100))\n",
    "print(\"Balanced accuracy mean {:.2f} %\".format(np.mean(balanced_accuracies)*100))\n",
    "print(\"Sensitivity mean {:.2f} %\".format(np.mean(sensitivities)*100))\n",
    "print(\"Specificity mean {:.2f} %\".format(np.mean(specificities)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.0h:23.0m:54.650190353393555s\n",
      "Total time elapsed: 0.0h:27.0m:33.08427333831787s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Time: {}h:{}m:{}s\".format(\n",
    "            training_time//3600, (training_time//60)%60, training_time%60))\n",
    "print(\"Total time elapsed: {}h:{}m:{}s\".format(\n",
    "            total_time//3600, (total_time//60)%60, total_time%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (postal)",
   "language": "python",
   "name": "postal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
