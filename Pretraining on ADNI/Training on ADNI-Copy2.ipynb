{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Conv3D, Flatten, Dense, MaxPooling3D, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "\n",
    "from config import *\n",
    "from utils import specificity\n",
    "from utils import sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensorflow settings\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session, get_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.visible_device_list = \"7\"\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtype = np.float32\n",
    "result_dir = \"/analysis/share/Ritter/models/fabi/ADNI/pretraining_paper/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load hdf5 files and extract columns\n",
    "train_h5 = h5py.File(\"/analysis/share/ADNI/HDF5_files/train_2yr_and_unique_screening-921_subjects-96_114_96_shape-masked.h5\", 'r')\n",
    "holdout_h5 = h5py.File(\"/analysis/share/ADNI/HDF5_files/test_2yr_and_unique_screening-150_subjects-96_114_96_shape-masked.h5\", 'r')\n",
    "\n",
    "X_train, y_train, files_train = train_h5['X'], train_h5['y'], train_h5[\"files\"]\n",
    "X_holdout, y_holdout = holdout_h5['X'], holdout_h5['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data to numpy arrays\n",
    "X_train = np.expand_dims(np.array(X_train, dtype=dtype), 4)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_holdout = np.expand_dims(np.array(X_holdout, dtype=dtype), 4)\n",
    "y_holdout = np.array(y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total datset length: {}\".format(len(y_train)))\n",
    "print(\"Number of healthy controls: {}\".format(len(np.array(y_train)[np.array(y_train)==0.])))\n",
    "print(\"Number of AD patients: {}\".format(len(np.array(y_train)[np.array(y_train)==1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ADNIDataset(Sequence):\n",
    "    def __init__(self, X, y, transform=None, batch_size=4, z_factor=None, shuffle=True, mask=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.z_factor = z_factor\n",
    "        self.shuffle = shuffle\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # add BET\n",
    "        image = np.expand_dims(self.X[idx * self.batch_size:(idx + 1) * self.batch_size],5)\n",
    "        label = np.array(self.y[idx * self.batch_size:(idx + 1) * self.batch_size], dtype=np.int8)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            for i in range(image.shape[0]):\n",
    "                image[i] *= self.mask\n",
    "        \n",
    "        for transformation in self.transform:\n",
    "            image = transformation(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.X, self.y = shuffle(self.X, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# images are already masked\n",
    "mask = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SixtyFourNet(input_shape, drop_rate=0., weight_dcay=0.):\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(64, kernel_size=3, \n",
    "                     input_shape=(input_shape[1], input_shape[2], input_shape[3], 1), \n",
    "                     activation='elu', padding='valid', name='Conv_1'))\n",
    "    model.add(MaxPooling3D(pool_size=3, name='Pool_1'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    model.add(Conv3D(64, kernel_size=3, padding='valid', activation='elu', name='Conv_2'))\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=3, name='Pool_2'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    model.add(Conv3D(64, kernel_size=3, padding='valid', activation='elu', name='Conv_3',\n",
    "                     kernel_regularizer=l2(weight_dcay)))\n",
    "    model.add(Conv3D(64, kernel_size=3, padding='valid', activation='elu', name='Conv_4',\n",
    "                     kernel_regularizer=l2(weight_dcay)))\n",
    "    model.add(MaxPooling3D(pool_size=3, name='Pool_4'))\n",
    "    model.add(Dropout(drop_rate))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files_train = np.array(files_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_train = [item[53:53+10].decode('utf-8') for item in files_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize min-max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(X_train)):\n",
    "    X_train[i] -= np.min(X_train[i])\n",
    "    X_train[i] /= np.max(X_train[i])\n",
    "    \n",
    "for i in range(len(X_holdout)):\n",
    "    X_holdout[i] -= np.min(X_holdout[i])\n",
    "    X_holdout[i] /= np.max(X_holdout[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize by voxel and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GSS = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=22)\n",
    "indices = list(GSS.split(names_train, groups=names_train))[0]\n",
    "train_idx, val_idx = indices[0], indices[1]\n",
    "\n",
    "mean = np.mean(X_train[train_idx], axis=0)\n",
    "std = np.std(X_train[train_idx], axis=0)\n",
    "X_train = (X_train - mean) / (std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle indices test\n",
    "np.random.shuffle(train_idx)\n",
    "np.random.shuffle(val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[4][:,:,46,0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(X_train[:,40,71,40,0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[4][:,:,46, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training args\n",
    "lr = 0.002\n",
    "lr_decay = 0.\n",
    "\n",
    "num_trials = 10\n",
    "patience = 10\n",
    "store_models = True\n",
    "\n",
    "best_epoch_acc = []\n",
    "final_epoch_sens = []\n",
    "final_epoch_spec = []\n",
    "\n",
    "for i in range(num_trials):\n",
    "    # init model\n",
    "    model = SixtyFourNet(drop_rate=0.3, weight_dcay=0.04, input_shape=X_train.shape)\n",
    "    opti = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opti, metrics=['accuracy'])\n",
    "    \n",
    "    #callbacks\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, verbose=0, mode='auto')\n",
    "    model_checkpoint = ModelCheckpoint(os.path.join(result_dir, \"model_trial_%i.h5\" %i), monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "    if store_models:\n",
    "        callbacks = [earlystopping, model_checkpoint]\n",
    "    else:\n",
    "        callbacks = [earlystopping]\n",
    "    \n",
    "    # Start training\n",
    "    history = model.fit(\n",
    "        X_train[train_idx],\n",
    "        y_train[train_idx],\n",
    "        epochs=num_epochs,\n",
    "        batch_size=b,\n",
    "        validation_data=(X_train[val_idx],y_train[val_idx]),\n",
    "        callbacks=callbacks,\n",
    "        shuffle=True\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(11, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history[\"loss\"])\n",
    "    plt.plot(history.history[\"val_loss\"])\n",
    "    plt.legend([\"Train\", \"Val\"])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history[\"acc\"])\n",
    "    plt.plot(history.history[\"val_acc\"])\n",
    "    plt.legend([\"Train\", \"Val\"])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict(X_train[val_idx], batch_size=1).flatten() # predict and turn into 1-dimensional array\n",
    "    sens = sensitivity(y_train[val_idx], y_pred>0.5)\n",
    "    spec = specificity(y_train[val_idx], y_pred>0.5)\n",
    "    # Store results\n",
    "    best_epoch = np.argmax(history.history[\"val_acc\"])\n",
    "    best_epoch_acc.append(history.history[\"val_acc\"][best_epoch])\n",
    "    final_epoch_sens.append(sens)\n",
    "    final_epoch_spec.append(spec)\n",
    "    # Print results\n",
    "    print(\"Final epoch results:\")\n",
    "    print(\"Balanced Accuracy: {:.2f} %\".format(((sens + spec) * 100) / 2))\n",
    "    print(\"Sensitivity: {:.2f} %\".format(sens * 100))\n",
    "    print(\"Specificity: {:.2f} %\".format(spec * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All best val accuracies: {}\".format(best_epoch_acc))\n",
    "print(\"Average best validation accuracy over {} trials: {:.5f} (std: {:.5f})\".format(num_trials, np.mean(best_epoch_acc), np.std(best_epoch_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final epochs sensitivity: {}\".format(final_epoch_sens))\n",
    "print(\"Final epochs specificity: {}\".format(final_epoch_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Time: {}h:{}m:{}s\".format(\n",
    "            training_time//3600, (training_time//60)%60, training_time%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(history.history[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (postal)",
   "language": "python",
   "name": "postal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
