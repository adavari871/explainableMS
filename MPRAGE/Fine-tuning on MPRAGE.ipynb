{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fine-tuning we have 3 datasets: train, validation and holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "# keras\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import Sequence\n",
    "import keras.backend as K\n",
    "\n",
    "from config import *\n",
    "from utils import specificity, sensitivity, balanced_accuracy, IntensityRescale, sagittal_flip, translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start timing\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#percent = 0.5\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.per_process_gpu_memory_fraction = percent\n",
    "config.gpu_options.visible_device_list = \"4\"\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "zero_one_normalize = True\n",
    "dtype = np.float32\n",
    "result_dir = \"/analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $result_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load hdf5 files and extract columns\n",
    "train_h5 = h5py.File('/analysis/share/Ritter/MS/CIS/train_dataset_MPRAGE.h5', 'r')\n",
    "holdout_h5 = h5py.File('/analysis/share/Ritter/MS/CIS/holdout_dataset_MPRAGE.h5', 'r')\n",
    "\n",
    "X_train, y_train = train_h5['X'], train_h5['y']\n",
    "X_holdout, y_holdout = holdout_h5['X'], holdout_h5['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert data to numpy arrays\n",
    "X_train = np.array(X_train, dtype=dtype)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_holdout = np.array(X_holdout, dtype=dtype)\n",
    "y_holdout = np.array(y_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total datset length: {}\".format(len(y_train)))\n",
    "print(\"Number of healthy controls: {}\".format(len(np.array(y_train)[np.array(y_train)==0.])))\n",
    "print(\"Number of MS patients: {}\".format(len(np.array(y_train)[np.array(y_train)==1.])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CISDataset(Sequence):\n",
    "    def __init__(self, X, y, transform=None, batch_size=4, z_factor=None, shuffle=True, mask=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.z_factor = z_factor\n",
    "        self.shuffle = shuffle\n",
    "        self.mask = mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.y)/self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # add BET\n",
    "        image = np.expand_dims(self.X[idx * self.batch_size:(idx + 1) * self.batch_size],5)\n",
    "        #label = np.array((batch_idx['label'] == \"MS\")* 1, dtype=np.int8) \n",
    "        label = np.array(self.y[idx * self.batch_size:(idx + 1) * self.batch_size], dtype=np.int8)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            for i in range(image.shape[0]):\n",
    "                image[i] *= self.mask\n",
    "        \n",
    "        for transformation in self.transform:\n",
    "            image = transformation(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.X, self.y = shuffle(self.X, self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intensity = IntensityRescale(masked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if zero_one_normalize:\n",
    "    cis_data = CISDataset(X_train, y_train, transform=[intensity], batch_size=4)\n",
    "else:\n",
    "    cis_data = CISDataset(X_train, y_train, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Normalized between zero and 1\")\n",
    "plt.hist(cis_data[4][0][0].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Scan image\")\n",
    "plt.imshow(np.squeeze(cis_data[4][0][0])[:,:,42], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_model(path, finetune=True, up_to=7):\n",
    "    model = load_model(path)\n",
    "    model.load_weights(path)\n",
    "    if finetune:\n",
    "        for layer in model.layers[:up_to]:\n",
    "            layer.trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv_1 (Conv3D)              (None, 94, 112, 94, 64)   1792      \n",
      "_________________________________________________________________\n",
      "Pool_1 (MaxPooling3D)        (None, 31, 37, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 31, 37, 31, 64)    0         \n",
      "_________________________________________________________________\n",
      "Conv_2 (Conv3D)              (None, 29, 35, 29, 64)    110656    \n",
      "_________________________________________________________________\n",
      "Pool_2 (MaxPooling3D)        (None, 9, 11, 9, 64)      0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 9, 11, 9, 64)      0         \n",
      "_________________________________________________________________\n",
      "Conv_3 (Conv3D)              (None, 7, 9, 7, 64)       110656    \n",
      "_________________________________________________________________\n",
      "Conv_4 (Conv3D)              (None, 5, 7, 5, 64)       110656    \n",
      "_________________________________________________________________\n",
      "Pool_4 (MaxPooling3D)        (None, 1, 2, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 1, 2, 1, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 333,889\n",
      "Trainable params: 333,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load model weights\n",
    "model_path = \"/analysis/share/Ritter/models/fabi/ADNI/pretraining_paper/\"\n",
    "model = init_model(model_path, finetune=False, up_to=None)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_weights(model):\n",
    "    # Visualize weights\n",
    "    W = model.layers[0].get_weights()[0]\n",
    "    W = np.squeeze(W)[:,:,2]\n",
    "    print(\"W shape : \", W.shape)\n",
    "\n",
    "    print(\"Weights mean {}\".format(W.mean()))\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.title('conv1 weights')\n",
    "    plt.imshow(make_mosaic(W, 2, 2), cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.ma as ma\n",
    "def make_mosaic(imgs, nrows, ncols, border=1):\n",
    "    \"\"\"\n",
    "    Given a set of images with all the same shape, makes a\n",
    "    mosaic with nrows and ncols\n",
    "    \"\"\"\n",
    "    nimgs = imgs.shape[0]\n",
    "    imshape = imgs.shape[1:]\n",
    "    \n",
    "    mosaic = ma.masked_all((nrows * imshape[0] + (nrows - 1) * border,\n",
    "                            ncols * imshape[1] + (ncols - 1) * border),\n",
    "                            dtype=np.float32)\n",
    "    \n",
    "    paddedh = imshape[0] + border\n",
    "    paddedw = imshape[1] + border\n",
    "    for i in range(nimgs):\n",
    "        row = int(np.floor(i / ncols))\n",
    "        col = i % ncols\n",
    "        \n",
    "        mosaic[row * paddedh:row * paddedh + imshape[0],\n",
    "               col * paddedw:col * paddedw + imshape[1]] = imgs[i]\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model layer 1 weights:\n",
      "W shape :  (3, 3, 64)\n",
      "Weights mean -0.003789090784266591\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIQAAABrCAYAAAAGj1lyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAHNFJREFUeJzt3XmUleW15/HfDlQxIyLIjICADApBCGLUyNXrGKIkRoM3Xo33IuLSjt7ktq2xO912vFlt7BU7thqXXoN6l2IMg6JxTJyQRRAEQ0SFAGESinmeh91/nJe2LKqe/RIKTlXO97MWizpnb/Z5zqnnPO97Nu/7HnN3AQAAAAAAoHR8qdgDAAAAAAAAwLFFQwgAAAAAAKDE0BACAAAAAAAoMTSEAAAAAAAASgwNIQAAAAAAgBJDQwgAAAAAAKDE0BACAAA4iszsETP7bzlznzCze472mAAAAGgIAQCAkmFmHcxsipmtNDM3s25H+zHdfay7/6Q2amVj7lkbtQAAQGmjIQQAAErJAUmvSrqi2AMBAAAoJhpCAACgaMysi5lNMrO1ZrbezB7M7v+Smf1XM1tqZmvM7CkzOy6LdcuOlLnOzJaZ2TozuyuLdTSznWbWutJjDMpyytx9tbs/LGlmjrFdb2YvVrq90Myeq3R7uZl9Ofu5j5m9YWYbzGy+mV1VKe8Lp4GZ2e1mtio7Sml0NUf9HG9mvzWzrWY2w8xOzv7du1n8j2a2zcy+Y2ZtzOwlM9uUPfZUM2P/DgAAhNhhAAAARWFmDSS9JGmppG6SOkl6Ngt/L/vzd5J6SGou6cEqJc6WdIqk8yX92Mz6uvtKSdP1xSOA/kHSBHffe5hDfEfSOVlzqoOkMklnZWM/OKa5ZtZM0huSnpF0oqSrJT1sZv2rec4XS/qBpL+X1FPSudU87tWS7pZ0vKSFkv5Nktz9a1l8oLs3d/dfS/qhpBWS2kpqJ+lHkvwwnycAAChBNIQAAECxDJXUUdJ/dvft7r7L3d/LYt+V9HN3X+zu2yTdKWmUmTWs9O/vdved7v5HSX+UNDC7/xkVmioyM5M0KrvvsLj7YklbJX1ZhcbNa5I+M7M+2e2p7n5A0ghJS9x9nLvvc/fZkiZK+nY1Za+SNM7d57n7DhUaP1VNcvf33X2fpKezx6/JXkkdJJ3k7nvdfaq70xACAAAhGkIAAKBYukhamjU+quqowpFDBy2V1FCFo2AOqqj08w4VjtiRpAmSzjSzjpK+psIRM1P/yjG+I2l4VucdSW+r0Aw6N7stSSdJOiM7bWuTmW1SoaHVvobntbzS7eXV5NT0vKpznwpHEb1uZovN7I7oCQEAAEiFHSsAAIBiWC6pq5k1rKYptFKFRstBXSXtk7RaUudUUXffZGavq3A0Tl9J44/gqJl3JH1DUndJP5V0sNlzpj4/hW25pHfc/YIc9Vbpi+Pv8leOS5Lk7ltVOG3sh9kpam+Z2Ux3//2R1AUAAH/7OEIIAAAUy/sqNEj+l5k1M7PGZnZWFhsv6V/MrLuZNVehGfPrGo4mqs4zkq5V4VpCXzhdzMwaS2qU3WyU3a7JOypcx6iJu69Q4UijiyWdIGlOlvOSpN5m9o9mVpb9+YqZ9a2m3nOSrjezvmbWVNKPcz6fg1arcE2lg89lhJn1zE6N2yJpf/YHAAAgiYYQAAAoCnffr8LRNz0lLVPh4sjfycK/kvQfkt6V9BdJuyT9p8MoP0VSL0mrs2sMVbZT0rbs50+z2zWNcUGWOzW7vUXSYknTsvEfPErnQhWuVbRShVO+7tXnTafK9V6R9ICkt1Q41Wt6Ftqd83n9D0lPZqemXZU9x99lY5wu6WF3fztnLQAAUMKM6w4CAAAUR3YU0UeSGh3G0U8AAABHjCOEAAAAjiEz+6aZlZvZ8SocSfQizSAAAHCs0RACAAA4tm6UtFbSIhWu93NTcYcDAABKEaeMAQAAAAAAlBiOEAIAAAAAACgxDYv1wOXl5d60adNkTuvWrZPxLVu2hI/ToUOHMGfnzhq/XESStHt3/MUfeY60KnwjbM127doV1igvL0/G9+zZc8Q1pHisGzduDGu0bds2zIlet82bN4c1ysrKwpxoLm3dujWskee1bd++fTK+YMGCsEarVq3CnOh3+KUvxb3eaN5LUqNGh3xBzmE/Tp73T7NmzZLxPPPtxBNPTMYrKirCGh07dgxzojmZZz7mmdfR+yfPa5JnLNHvJ1qnJWnfvvjSJw0aNAhzInnWrhNOOCEZX758eVjjuOOOC3O2b9+ejOfZPrVr1y4Z37ZtWzIu5Xtdo/dxtN5L+bZx0ZzM8/tr3Dj17e8FmzZtSsbzbHv27t0b5kR15s+fH9bo3LlzmLNy5cpkPM/7OM9rGz3naB2W4nkvxXMlz5oSzdk8r8mGDRvCnGgbluf5tmzZMsyJHH/88WFOnjU/2q7neX/lec6Rrl27hjnLli0Lc6K1OM+cXbNmTZgT7T8sXbo0rFEbn1nyPJ8mTZok43nWtjz7vNH+7Pr168MaeeZb9F7Ps++dZx9kx44dR1wjz/OJPsvlWauj116S1q5de8SPE70mktS8efNkvDY+IzdsGLcj8mw3osfJs6+TZ52N3oN5Xtc8z3n//v3JePTekaSVK1euc/dwp6hoDaGmTZvq3HPPTeZceeWVyfibb74ZPs6dd94Z5sybNy8ZX7x4cVgjzxsiWkg+/vjjsEb37t2T8TwbrS5duoQ50USdOHFiWOOmm+JLIkSv28svvxzWyLNwjho1KhmfOnVqWCPPzssdd9yRjA8fPjysMXLkyDAnmgfRYiVJc+fODXN69uyZjOdZoBctWhTmDB06NBn/zW9+E9a4+eabk/Gf/exnYY177rknzHnxxReT8TxNpVdeeSXMGT16dDI+efLksEbUcJCkJUuWJOODBg0Ka+TZMcyzoxvp1q1bmHPttdcm47fddltY45JLLglzZsyYkYzn2T7deuutyfi0adPCGnmaVyeffHIynqeplKch9NxzzyXjeZoj/fv3D3NeeOGFZHzs2LFhjVWrVoU5N954YzKeZz2/9957w5y77747Gc/T4MrzATxqig8bNiysEc17Kd6un3766WGNXr16JeN5tvvPPPNMmBNtw/I83wsuuCDMid5j0f6ulG87GG3X+/btG9aYPn16mBN5+OGHw5xomy1JI0aMSMaHDBkS1njooYfCnFtuuSUZv+GGG8Ia11xzTTL++uuvhzWifSFJOvXUU5PxPP/5VRufn8aNGxfW6NevX5gT7c/m2fdet25dmPPhhx8m43n2Y/r06RPmRJ/l8uzH3H777WHOI488csSP88EHH4Q5Z599djL+5z//OawRNTai/8iTpMGDB4c5UTM0z39iT5o0Kcw57bTTkvE5c+aENdq0aRPmRP+p0bt377DGXXfdFTcGlPOUMTO72Mzmm9lCMzvkE6+ZNTKzX2fxGWbWLU9dAAAAAAAAHHthQ8jMGkh6SNIlkvpJutrMqrZ8/1nSRnfvKel+Fb5CFQAAAAAAAHVQniOEhkpa6O6L3X2PpGclXV4l53JJT2Y/T5B0vuU5UQ8AAAAAAADHXJ6GUCdJla/CuSK7r9ocd98nabOkQ04INLMxZjbLzGbluUgYAAAAAAAAal+ehlB1R/pUvbpknhy5+6PuPsTdh+S5+jkAAAAAAABqX56G0ApJlb+WqrOkqt+R+v9zzKyhpOMkxd/3CQAAAAAAgGMuT0NopqReZtbdzMoljZI0pUrOFEnXZT9/W9Kbnuc7agEAAAAAAHDMNYwS3H2fmd0i6TVJDST9yt3nmdn/lDTL3adIelzSf5jZQhWODBoV1W3atKkGDBiQzGnXrl0yvnJl1QOVDvXuu++GOdu2bUvGFyxYcMQ1JGn06NHJ+IQJE8IaI0aMSMZffPHFsEaPHj3CnGeeeSYZj353kjR06NAwp6KiIhk/66yzwhp5fj8fffRRMt65c+ewRosWLcKc1157LRkfP358WCPPPJg9e3Yyftddd4U1Hn/88TBn+vTpyfj3v//9sEae13bKlKo95i9q3LhxWGPjxo3J+AknHHJZs0Pkee379OmTjO/bty+scfPNN4c5r776ajIerY+SdPXVV4c51157bTLev3//sMaNN94Y5ixatCgZz7OmvPHGG2HOE088kYx/85vfDGt8+umnYU40r7t16xbWWLNmTTL+9a9/Paxx/PHHhzlz585Nxlu3bh3WyPOa9OrVKxnPs+3Zu3dvmDNw4MBkPM/zmTlzZpgTybNejBs3LsyJ3mOrV68Oa5x++ulhzi9+8YtkvKysLKwxduzYMOett95Kxrds2RLWaNOmTTKeZ//vd7/7XZgT7VOdc845YY327duHOe+//34ynmc/Zvny5WHOeeedl4y/9NJLYY0xY8aEOZs3b07Go226JJ100klhzpIlS5Lx6PlKUsuWLcOcDRvSJzV07949rDF48OBkfOLEiWGNPPP65JNPTsY7dap6mddD5dlPifZnFy5cGNaIfn+SdNVVVyXjvXv3Dmvs3LkzzOnYsWMyfsUVV4Q1mjZtGuaMHDkyGX/22WfDGi+88EKYE32m/N73vhfW+MlPfhLmrF+/PhnPs12fNWtWMt6oUaOwxm9/+9swZ/fu3cl4njUnz751v35Vv2z9i+65556wxre+9a0wJ9q/W7duXVgjr7AhJEnu/rKkl6vc9+NKP++SdGWtjQoAAAAAAABHTZ5TxgAAAAAAAPA3hIYQAAAAAABAiaEhBAAAAAAAUGJoCAEAAAAAAJQYGkIAAAAAAAAlhoYQAAAAAABAicn1tfNHw/79+7Vly5Zkzr59+5LxVq1ahY+zffv2MKe8vDwZ37NnT1hj0aJFYc7999+fjH/1q18NayxYsCAZ7927d1ijY8eOYc43vvGNI36cl156Kczp1q1bMj579uywxpgxY8Kchg3TU/2pp54Ka1x11VVhzvjx45PxadOmhTVGjhwZ5syaNSsZf/rpp8Man332WZjTokWLZLxz58618jibNm1Kxnft2hXWiNaTaK5J0t69e8OcU045JRnv0qVLWGPChAlhzs6dO5PxYcOGhTWi10SSbr311mR83bp1YY08z3n16tXJ+N133x3W2LZtW5gzZMiQZPzBBx8Ma3Tq1CnMiV7bCy+8MKwxf/78ZHzr1q1hjV69eoU5ZpaM53ldO3ToEOYcOHAgGW/btm1Y49NPPw1zmjRpkowvWbIkrNG9e/cwJ7Jy5cowZ/fu3WFOtKacfPLJYY05c+aEOWeccUYy3qBBg7DGRx99FOZE7/VPPvkkrNG8efNk3N3DGk8++WSYM3Xq1GS8Xbt2YY08+zplZWXJ+GOPPRbWGDBgQJgTvQej342Ub5vdo0ePZDzPa9+nT58wZ+HChcn4o48+Gtaojf2HH/zgB2GN6PPGmWeeGdZYsWJFmBO9Jps3bw5r5NmfjbbJN910U1hj7ty5YU60b/bTn/40rHHNNdeEOePGjUvGr7/++rDG0qVLw5zoc1qe/eZomy1JU6ZMScbPPvvssEaesUyePDkZz/PZtaKiIhmPtk1Svv3zli1bJuNr164Na0SfF6V4zd+wYUNY49JLLw1zJk2alIxHfZLDwRFCAAAAAAAAJYaGEAAAAAAAQImhIQQAAAAAAFBiaAgBAAAAAACUGBpCAAAAAAAAJSZsCJlZFzN7y8w+MbN5ZnbIV9KY2XAz22xmH2Z/fnx0hgsAAAAAAIAjledr5/dJ+qG7zzazFpI+MLM33P3jKnlT3X1E7Q8RAAAAAAAAtSk8QsjdV7n77OznrZI+kdTpaA8MAAAAAAAAR4e5e/5ks26S3pV0qrtvqXT/cEkTJa2QtFLSv7r7vGr+/RhJYySpa9eug5cuXXoEQwcAAAAAAEBlZvaBuw+J8nJfVNrMmqvQ9LmtcjMoM1vSSe4+UNL/lfR8dTXc/VF3H+LuQ9q2bZv3oQEAAAAAAFCLcjWEzKxMhWbQ0+4+qWrc3be4+7bs55cllZlZm1odKQAAAAAAAGpFnm8ZM0mPS/rE3X9eQ077LE9mNjSru742BwoAAAAAAIDakedbxs6S9I+S/mRmH2b3/UhSV0ly90ckfVvSTWa2T9JOSaP8cC5OBAAAAAAAgGMmbAi5+3uSLMh5UNKDtTUoAAAAAAAAHD25LyoNAAAAAACAvw00hAAAAAAAAEoMDSEAAAAAAIASk+ei0kdFRUWF7rvvvmTO9u3bk/G+ffuGj7NmzZowp3Pnzsl4s2bNwhrz588Pcxo1apSMr1u3LqzRvHnzZLxVq1ZhjcmTJ4c5Z555ZjLepEmTsEa3bt3CnOh1KysrC2u0aNEizFm4cGEyPnz48LBGnuukT5s2LRlv3bp1WGPjxo1hTkVFRTJ+/vnnhzV27twZ5rRt2/aIxiFJ+/fvD3NWr16djO/atSusMXjw4GT8ww8/TMYlacCAAWHOBRdckIzfcccdYY3zzjsvzNm2bVsyXl5eHtbI87pt3bo1zIm8/fbbYU7DhunNzaBBg8IaBw4cCHOmTp2ajHfs2DGssXv37jBnx44dyfjAgQPDGtFr37Vr17BGnnkQPU7jxo3DGtH2WIq3T9F2RZImTpwY5kTbhQ4dOoQ18myfLr300jAHAACgPuMIIQAAAAAAgBJDQwgAAAAAAKDE0BACAAAAAAAoMTSEAAAAAAAASgwNIQAAAAAAgBJDQwgAAAAAAKDE0BACAAAAAAAoMTSEAAAAAAAASkzDYj1weXm5OnTokMxZs2ZNMt66devwcbZu3RrmPP/888n4LbfcEtZYvnx5mDNgwIBkfNCgQWGNxx57LBkfO3ZsWOPAgQNhznvvvZeMjxgxIqwxfvz4MKdTp07JeNeuXcMa69atC3NOOOGEZLy8vDysEc1HSerevXsynuf5rF69Oszp1atXMn7iiSeGNfbs2RPmNGyYXiI+++yzsMaOHTvCnNNOOy0ZX7RoUVhj/fr1yfjgwYPDGhUVFWFO9N5o3759WCPP84nmdd++fcMaS5YsCXOi3/Htt98e1nD3MOftt99Oxi+77LKwxsaNG8Ocxo0bJ+MDBw4Ma8yaNSvMiZ7ztm3bwhoXXXRRMh5tMyTp8ccfD3MWL16cjJeVlYU11q5dG+bccMMNyfj8+fPDGj169AhzFixYkIznWauvuOKKMAcAAOBvXa4jhMxsiZn9ycw+NLND9pSt4AEzW2hmc83s9NofKgAAAAAAAGrD4Rwh9HfuXtN/WV8iqVf25wxJv8z+BgAAAAAAQB1TW9cQulzSU17wB0mtzCx9PhgAAAAAAACKIm9DyCW9bmYfmNmYauKdJFW+iM6K7L4vMLMxZjbLzGZt2bLl8EcLAAAAAACAI5b3lLGz3H2lmZ0o6Q0z+9Td360Ut2r+zSFX3HT3RyU9Kkk9evSIr0IKAAAAAACAWpfrCCF3X5n9vUbSZElDq6SskNSl0u3OklbWxgABAAAAAABQu8KGkJk1M7MWB3+WdKGkj6qkTZF0bfZtY8MkbXb3VbU+WgAAAAAAAByxPKeMtZM02cwO5j/j7q+a2VhJcvdHJL0s6VJJCyXtkHR9VHT37t1asmRJMqdx48bJ+IwZM8LB9+zZM8wZOXJkMj59+vSwRr9+/cKctWvXJuM9evQIa4waNSoZ37hxY1hj6tSpYc4DDzyQjDdq1CisceONN4Y59957bzLet2/fsMZxxx0X5syePTsZz+Z3UtOmTcOcxYsXJ+N79uwJa8ybNy/MGT58eDL+1FNPhTUuv/zyMGfZsmVhTiTPe7Br165HPI6OHTsm423atAlr7N+/P8yZOXNmMt6gQYOwxle+8pUwJ5pLFRUVYY08r9sZZ6S/EHLNmjVhjWHDhoU5Xbp0ScajNUeSRo8eHea0bNkyGd+wYUNY45xzzglzFi1alIzPmTMnrBG9tsuXL0/GpXzP56KLLkrG9+7dG9Zwj8/y3rx5czK+bl1NX1T6ud69e4c5O3fuTMb79+8f1sizzrZr1y7MAQAAqM/ChpC7L5Y0sJr7H6n0s0u6uXaHBgAAAAAAgKOhtr52HgAAAAAAAPUEDSEAAAAAAIASQ0MIAAAAAACgxNAQAgAAAAAAKDE0hAAAAAAAAEoMDSEAAAAAAIASQ0MIAAAAAACgxJi7F+eBzdZKWlrl7jaS1hVhOMBfizmL+oY5i/qGOYv6hjmL+oY5i/qGORs7yd3bRklFawhVx8xmufuQYo8DyIs5i/qGOYv6hjmL+oY5i/qGOYv6hjlbezhlDAAAAAAAoMTQEAIAAAAAACgxda0h9GixBwAcJuYs6hvmLOob5izqG+Ys6hvmLOob5mwtqVPXEAIAAAAAAMDRV9eOEAIAAAAAAMBRRkMIAAAAAACgxNSJhpCZXWxm881soZndUezxAFWZWRcze8vMPjGzeWZ2a3Z/azN7w8z+nP19fLHHClRmZg3MbI6ZvZTd7m5mM7I5+2szKy/2GIGDzKyVmU0ws0+z9fZM1lnUZWb2L9l+wUdmNt7MGrPOoi4xs1+Z2Roz+6jSfdWuq1bwQPaZbK6ZnV68kaNU1TBn78v2Deaa2WQza1Updmc2Z+eb2UXFGXX9VfSGkJk1kPSQpEsk9ZN0tZn1K+6ogEPsk/RDd+8raZikm7N5eoek37t7L0m/z24Ddcmtkj6pdPteSfdnc3ajpH8uyqiA6v1C0qvu3kfSQBXmLuss6iQz6yTp+5KGuPupkhpIGiXWWdQtT0i6uMp9Na2rl0jqlf0ZI+mXx2iMQGVP6NA5+4akU919gKQFku6UpOzz2ChJ/bN/83DWX0BORW8ISRoqaaG7L3b3PZKelXR5kccEfIG7r3L32dnPW1X4kNJJhbn6ZJb2pKSRxRkhcCgz6yzp65L+Pbttks6TNCFLYc6izjCzlpK+JulxSXL3Pe6+SayzqNsaSmpiZg0lNZW0SqyzqEPc/V1JG6rcXdO6ermkp7zgD5JamVmHYzNSoKC6Oevur7v7vuzmHyR1zn6+XNKz7r7b3f8iaaEK/QXkVBcaQp0kLa90e0V2H1AnmVk3SYMkzZDUzt1XSYWmkaQTizcy4BD/R9Ltkg5kt0+QtKnSBpX1FnVJD0lrJY3LTnP8dzNrJtZZ1FHu/pmk/y1pmQqNoM2SPhDrLOq+mtZVPpehPvgnSa9kPzNnj1BdaAhZNff5MR8FkIOZNZc0UdJt7r6l2OMBamJmIyStcfcPKt9dTSrrLeqKhpJOl/RLdx8kabs4PQx1WHbdlcsldZfUUVIzFU65qYp1FvUF+wmo08zsLhUu5fH0wbuqSWPOHoa60BBaIalLpdudJa0s0liAGplZmQrNoKfdfVJ29+qDh9Jmf68p1viAKs6SdJmZLVHhVNzzVDhiqFV2aoPEeou6ZYWkFe4+I7s9QYUGEess6qq/l/QXd1/r7nslTZL0VbHOou6raV3lcxnqLDO7TtIISd9194NNH+bsEaoLDaGZknpl38hQrsJFoaYUeUzAF2TXXnlc0ifu/vNKoSmSrst+vk7SC8d6bEB13P1Od+/s7t1UWFffdPfvSnpL0rezNOYs6gx3r5C03MxOye46X9LHYp1F3bVM0jAza5rtJxycs6yzqOtqWlenSLo2+7axYZI2Hzy1DCgmM7tY0n+RdJm776gUmiJplJk1MrPuKlwQ/f1ijLG+ss+ba0UchNmlKvzPdQNJv3L3fyvykIAvMLOzJU2V9Cd9fj2WH6lwHaHnJHVVYcfwSneveuE+oKjMbLikf3X3EWbWQ4UjhlpLmiPpGnffXczxAQeZ2ZdVuAh6uaTFkq5X4T+vWGdRJ5nZ3ZK+o8IpDHMkjVbh+hWss6gTzGy8pOGS2khaLem/S3pe1ayrWWPzQRW+rWmHpOvdfVYxxo3SVcOcvVNSI0nrs7Q/uPvYLP8uFa4rtE+Fy3q8UrUmalYnGkIAAAAAAAA4durCKWMAAAAAAAA4hmgIAQAAAAAAlBgaQgAAAAAAACWGhhAAAAAAAECJoSEEAAAAAABQYmgIAQAAAAAAlBgaQgAAAAAAACXm/wE6CKPXoX/aeAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random model layer 1 weights:\n",
      "W shape :  (3, 3, 64)\n",
      "Weights mean 0.0008080286206677556\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize weights\n",
    "print(\"Pre-trained model layer 1 weights:\")\n",
    "visualize_weights(model)\n",
    "model_untrained = load_model(model_path)\n",
    "reset_weights(model_untrained)\n",
    "print(\"Random model layer 1 weights:\")\n",
    "visualize_weights(model_untrained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 96, 114, 96)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split training set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 9s 307ms/step - loss: 1.0286 - acc: 0.4673 - val_loss: 0.8644 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial0-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 155ms/step - loss: 0.9160 - acc: 0.5209 - val_loss: 0.9101 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.9610 - acc: 0.4941 - val_loss: 0.9218 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.9220 - acc: 0.4673 - val_loss: 0.8504 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.8876 - acc: 0.5298 - val_loss: 0.7818 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 181ms/step - loss: 0.8711 - acc: 0.5177 - val_loss: 0.8501 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.9180 - acc: 0.4762 - val_loss: 0.8245 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.8630 - acc: 0.4702 - val_loss: 0.8328 - val_acc: 0.1538\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.9242 - acc: 0.4762 - val_loss: 0.9816 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.8324 - acc: 0.5595 - val_loss: 0.7496 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 1.0115 - acc: 0.4941 - val_loss: 0.7847 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8418 - acc: 0.5506 - val_loss: 0.8974 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 179ms/step - loss: 0.8364 - acc: 0.5388 - val_loss: 0.8535 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.7884 - acc: 0.5416 - val_loss: 0.8670 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.8425 - acc: 0.5059 - val_loss: 0.8849 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.69231\n",
      "Epoch 00015: early stopping\n",
      "Trial 1\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 229ms/step - loss: 1.0019 - acc: 0.4644 - val_loss: 1.1719 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 1.0181 - acc: 0.4494 - val_loss: 0.7723 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30769 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial1-improvement-BEST.hdf5\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 1.0878 - acc: 0.4734 - val_loss: 0.9215 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 1.0000 - acc: 0.5388 - val_loss: 0.9011 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8702 - acc: 0.5327 - val_loss: 0.8174 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.9310 - acc: 0.3691 - val_loss: 0.8850 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8629 - acc: 0.4852 - val_loss: 0.8482 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 00007: early stopping\n",
      "Trial 2\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 220ms/step - loss: 1.0220 - acc: 0.4255 - val_loss: 1.0860 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial2-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.9330 - acc: 0.5713 - val_loss: 0.8436 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30769 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial2-improvement-BEST.hdf5\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.9966 - acc: 0.5445 - val_loss: 0.8877 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 1.0809 - acc: 0.4287 - val_loss: 0.8726 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.9054 - acc: 0.5774 - val_loss: 0.9455 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8470 - acc: 0.5298 - val_loss: 0.8212 - val_acc: 0.1538\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 178ms/step - loss: 0.8946 - acc: 0.4941 - val_loss: 0.8090 - val_acc: 0.1538\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8683 - acc: 0.4880 - val_loss: 0.8614 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.8087 - acc: 0.5269 - val_loss: 0.7461 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.8193 - acc: 0.5327 - val_loss: 0.7670 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8196 - acc: 0.4702 - val_loss: 0.7936 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 178ms/step - loss: 0.8297 - acc: 0.4584 - val_loss: 0.7837 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.8146 - acc: 0.4791 - val_loss: 0.8584 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 180ms/step - loss: 0.8205 - acc: 0.4523 - val_loss: 0.8137 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 00014: early stopping\n",
      "Trial 3\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 7s 234ms/step - loss: 1.0259 - acc: 0.4673 - val_loss: 0.9168 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial3-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.9587 - acc: 0.4970 - val_loss: 0.8976 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.30769\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.9054 - acc: 0.5209 - val_loss: 0.8528 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.30769\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8680 - acc: 0.5534 - val_loss: 0.8316 - val_acc: 0.3077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00004: val_acc did not improve from 0.30769\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 178ms/step - loss: 0.9221 - acc: 0.4584 - val_loss: 0.8212 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.30769\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 173ms/step - loss: 0.8901 - acc: 0.4820 - val_loss: 0.8139 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.30769\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 179ms/step - loss: 0.8355 - acc: 0.4880 - val_loss: 0.8452 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.30769\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.8564 - acc: 0.4316 - val_loss: 0.8142 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.30769\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8396 - acc: 0.4762 - val_loss: 0.7928 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.30769\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7993 - acc: 0.4852 - val_loss: 0.7455 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.30769 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial3-improvement-BEST.hdf5\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 4s 158ms/step - loss: 0.8489 - acc: 0.4405 - val_loss: 0.7955 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.7541 - acc: 0.5327 - val_loss: 0.7977 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.7733 - acc: 0.5148 - val_loss: 0.7903 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7604 - acc: 0.4909 - val_loss: 0.7702 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.7887 - acc: 0.4584 - val_loss: 0.7673 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.69231\n",
      "Epoch 00015: early stopping\n",
      "Trial 4\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 6s 232ms/step - loss: 1.0925 - acc: 0.5180 - val_loss: 1.3498 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.30769, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial4-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 156ms/step - loss: 0.9385 - acc: 0.6695 - val_loss: 0.8233 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.30769 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial4-improvement-BEST.hdf5\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.9437 - acc: 0.5180 - val_loss: 0.8782 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.9656 - acc: 0.4405 - val_loss: 0.8226 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8860 - acc: 0.5834 - val_loss: 0.8230 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8293 - acc: 0.5716 - val_loss: 0.9651 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.8353 - acc: 0.5002 - val_loss: 0.8606 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.8288 - acc: 0.5416 - val_loss: 0.8056 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7821 - acc: 0.5952 - val_loss: 0.8273 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7942 - acc: 0.5002 - val_loss: 0.7989 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.7964 - acc: 0.5774 - val_loss: 0.8015 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.8007 - acc: 0.5298 - val_loss: 0.7732 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.8178 - acc: 0.5238 - val_loss: 0.7745 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8024 - acc: 0.4734 - val_loss: 0.7822 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.7979 - acc: 0.5356 - val_loss: 0.7785 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.69231\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.8064 - acc: 0.5002 - val_loss: 0.7667 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.69231\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.8096 - acc: 0.4494 - val_loss: 0.7732 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.69231\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.8209 - acc: 0.4823 - val_loss: 0.7767 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.69231\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8280 - acc: 0.4137 - val_loss: 0.7299 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.69231\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.7914 - acc: 0.5002 - val_loss: 0.8360 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.69231\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7898 - acc: 0.5359 - val_loss: 0.7578 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.69231\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.8171 - acc: 0.5148 - val_loss: 0.9132 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.69231\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8444 - acc: 0.4376 - val_loss: 0.7975 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.69231\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7998 - acc: 0.5445 - val_loss: 0.7322 - val_acc: 0.5385\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.69231\n",
      "Epoch 00024: early stopping\n",
      "Trial 5\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 7s 240ms/step - loss: 0.9724 - acc: 0.5506 - val_loss: 0.8480 - val_acc: 0.6154\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.61538, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial5-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.9039 - acc: 0.5269 - val_loss: 0.7914 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.61538 to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial5-improvement-BEST.hdf5\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.9102 - acc: 0.4880 - val_loss: 0.8429 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.9059 - acc: 0.5506 - val_loss: 0.8182 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8906 - acc: 0.4791 - val_loss: 0.7797 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.8368 - acc: 0.5327 - val_loss: 0.8811 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.8321 - acc: 0.4880 - val_loss: 0.7780 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.8179 - acc: 0.5209 - val_loss: 0.7806 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7822 - acc: 0.4852 - val_loss: 0.7121 - val_acc: 0.7692\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.69231 to 0.76923, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial5-improvement-BEST.hdf5\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8747 - acc: 0.4108 - val_loss: 0.7975 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76923\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8437 - acc: 0.5655 - val_loss: 0.8577 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.76923\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.8499 - acc: 0.4376 - val_loss: 0.8402 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76923\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8304 - acc: 0.4823 - val_loss: 0.8829 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76923\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.8377 - acc: 0.4852 - val_loss: 0.7608 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.76923\n",
      "Epoch 00014: early stopping\n",
      "Trial 6\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 7s 247ms/step - loss: 0.9710 - acc: 0.5059 - val_loss: 0.8401 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial6-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.9483 - acc: 0.4405 - val_loss: 0.9133 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.9654 - acc: 0.4673 - val_loss: 0.8351 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 176ms/step - loss: 0.8499 - acc: 0.6102 - val_loss: 0.7993 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 177ms/step - loss: 0.8884 - acc: 0.4555 - val_loss: 0.7968 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.8456 - acc: 0.5448 - val_loss: 0.8191 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 7/200\n",
      "28/28 [==============================] - 5s 175ms/step - loss: 0.8827 - acc: 0.4523 - val_loss: 1.7405 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69231\n",
      "Epoch 8/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8872 - acc: 0.5059 - val_loss: 0.8711 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69231\n",
      "Epoch 9/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8860 - acc: 0.4880 - val_loss: 0.7967 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69231\n",
      "Epoch 10/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.8289 - acc: 0.5148 - val_loss: 0.8149 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69231\n",
      "Epoch 11/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.8390 - acc: 0.5148 - val_loss: 0.7825 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.69231\n",
      "Epoch 12/200\n",
      "28/28 [==============================] - 5s 163ms/step - loss: 0.8432 - acc: 0.4880 - val_loss: 0.8396 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.69231\n",
      "Epoch 13/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.8720 - acc: 0.3959 - val_loss: 0.8148 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69231\n",
      "Epoch 14/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.8248 - acc: 0.4494 - val_loss: 0.7806 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.69231\n",
      "Epoch 15/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8010 - acc: 0.5209 - val_loss: 0.7662 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.69231\n",
      "Epoch 16/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 0.8176 - acc: 0.4048 - val_loss: 0.7860 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.69231\n",
      "Epoch 17/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7965 - acc: 0.4941 - val_loss: 0.7705 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.69231\n",
      "Epoch 18/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.7769 - acc: 0.5030 - val_loss: 0.7648 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.69231\n",
      "Epoch 19/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.8053 - acc: 0.4880 - val_loss: 0.7588 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.69231\n",
      "Epoch 20/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.7888 - acc: 0.5238 - val_loss: 0.7754 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.69231\n",
      "Epoch 21/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.8201 - acc: 0.4048 - val_loss: 0.7760 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.69231\n",
      "Epoch 22/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.7441 - acc: 0.5388 - val_loss: 0.7564 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.69231\n",
      "Epoch 23/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7908 - acc: 0.4970 - val_loss: 0.7819 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.69231\n",
      "Epoch 24/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7746 - acc: 0.5059 - val_loss: 0.7688 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.69231\n",
      "Epoch 25/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.7684 - acc: 0.5030 - val_loss: 0.7395 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.69231\n",
      "Epoch 26/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7711 - acc: 0.5059 - val_loss: 0.7532 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.69231\n",
      "Epoch 27/200\n",
      "28/28 [==============================] - 5s 162ms/step - loss: 0.7631 - acc: 0.5684 - val_loss: 0.7442 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.69231\n",
      "Epoch 28/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.7933 - acc: 0.4970 - val_loss: 0.7316 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.69231\n",
      "Epoch 29/200\n",
      "28/28 [==============================] - 5s 168ms/step - loss: 0.7628 - acc: 0.5298 - val_loss: 0.7693 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.69231\n",
      "Epoch 30/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.7547 - acc: 0.4880 - val_loss: 0.7623 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.69231\n",
      "Epoch 31/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.7473 - acc: 0.4762 - val_loss: 0.7373 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.69231\n",
      "Epoch 32/200\n",
      "28/28 [==============================] - 5s 172ms/step - loss: 0.7634 - acc: 0.4941 - val_loss: 0.8658 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.69231\n",
      "Epoch 33/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.7839 - acc: 0.5088 - val_loss: 0.7521 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.69231\n",
      "Epoch 00033: early stopping\n",
      "Trial 7\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 7s 252ms/step - loss: 1.1421 - acc: 0.4345 - val_loss: 0.7781 - val_acc: 0.6923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial7-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 157ms/step - loss: 0.9104 - acc: 0.5388 - val_loss: 0.8686 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 161ms/step - loss: 0.9315 - acc: 0.5238 - val_loss: 0.8543 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.9920 - acc: 0.4048 - val_loss: 0.8342 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 167ms/step - loss: 0.8988 - acc: 0.5327 - val_loss: 0.8636 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 171ms/step - loss: 0.8608 - acc: 0.5298 - val_loss: 0.8100 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 00006: early stopping\n",
      "Trial 8\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 7s 252ms/step - loss: 1.2727 - acc: 0.4852 - val_loss: 0.8246 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial8-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 1.0863 - acc: 0.5388 - val_loss: 1.9985 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 1.0340 - acc: 0.5595 - val_loss: 0.8723 - val_acc: 0.4615\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 5s 166ms/step - loss: 0.9768 - acc: 0.4376 - val_loss: 0.8933 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.9203 - acc: 0.5298 - val_loss: 0.9890 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 164ms/step - loss: 0.9474 - acc: 0.5148 - val_loss: 0.8742 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 00006: early stopping\n",
      "Trial 9\n",
      "Epoch 1/200\n",
      "28/28 [==============================] - 8s 272ms/step - loss: 1.0015 - acc: 0.4494 - val_loss: 0.8214 - val_acc: 0.6923\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.69231, saving model to /analysis/fabiane/models/MS/pretrained/rebuild_64Net/new_script/MPRAGE/weights-augm-trial9-improvement-BEST.hdf5\n",
      "Epoch 2/200\n",
      "28/28 [==============================] - 4s 160ms/step - loss: 1.1144 - acc: 0.3394 - val_loss: 0.8478 - val_acc: 0.2308\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.69231\n",
      "Epoch 3/200\n",
      "28/28 [==============================] - 5s 170ms/step - loss: 0.8660 - acc: 0.5534 - val_loss: 1.1648 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.69231\n",
      "Epoch 4/200\n",
      "28/28 [==============================] - 4s 159ms/step - loss: 0.9453 - acc: 0.5238 - val_loss: 0.8791 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.69231\n",
      "Epoch 5/200\n",
      "28/28 [==============================] - 5s 165ms/step - loss: 0.9673 - acc: 0.4584 - val_loss: 0.8354 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.69231\n",
      "Epoch 6/200\n",
      "28/28 [==============================] - 5s 169ms/step - loss: 0.8785 - acc: 0.5356 - val_loss: 1.4586 - val_acc: 0.3077\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.69231\n",
      "Epoch 00006: early stopping\n",
      "Training Time: 0.0h:13.0m:0.6799447536468506s\n",
      "Validation final accuracies: \n",
      " [0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.5384615384615384, 0.6923076923076923, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077, 0.3076923076923077]\n",
      "Validation final accuracies mean: 0.36923076923076925\n",
      "Validation best accuracies: \n",
      " [0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.7692307692307693, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923, 0.6923076923076923]\n",
      "Validation best accuracies mean: 0.7\n",
      "Validation balanced accuracies: \n",
      " [0.5, 0.5, 0.5, 0.5, 0.5277777777777778, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "Validation balanced accuracies mean: 0.5027777777777778\n",
      "Validation final sensitivities: \n",
      " [1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Validation final sensitivities' mean: 0.85\n",
      "Validation final specificities: \n",
      " [0.0, 0.0, 0.0, 0.0, 0.5555555555555556, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Validation final specificities' mean: 0.15555555555555556\n"
     ]
    }
   ],
   "source": [
    "# training args\n",
    "lr = 0.001\n",
    "lr_decay = 0.003\n",
    "transforms = [intensity, sagittal_flip, translate]\n",
    "\n",
    "num_trials = 10\n",
    "store_models = True\n",
    "\n",
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "max_acc = []\n",
    "\n",
    "for i in range(num_trials):\n",
    "    print(\"Trial %i\" %i)\n",
    "    \n",
    "    # init model\n",
    "    model = init_model(model_path, finetune=False, up_to=None)    \n",
    "    opti = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # callbacks\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto')\n",
    "    if store_models:\n",
    "        result_path = os.path.join(result_dir, \"weights-augm-trial%i-improvement-BEST.hdf5\" %i)\n",
    "        model_checkpoint = ModelCheckpoint(result_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max', save_weights_only=True)\n",
    "        callbacks = [earlystop, model_checkpoint]\n",
    "    else:\n",
    "        callbacks = [earlystop]\n",
    "        \n",
    "    train_loader = CISDataset(X_train, y_train, transform=transforms, batch_size=b, shuffle=True)\n",
    "    val_loader = CISDataset(X_val, y_val, transform=[intensity], batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Start training\n",
    "    history = model.fit_generator(train_loader,\n",
    "                        epochs=num_epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=val_loader,\n",
    "                        callbacks=callbacks)\n",
    "    \n",
    "    # Evaluation\n",
    "    y_pred = model.predict_generator(val_loader)\n",
    "    #y_true = [item for sublist in [val_loader[batch_idx][1] for batch_idx in range(len(val_loader))] for item in sublist]\n",
    "    bal_acc = balanced_accuracy(y_val, y_pred>0.5)\n",
    "    sens = sensitivity(y_val, y_pred>0.5)\n",
    "    spec = specificity(y_val, y_pred>0.5)\n",
    "    # Store results\n",
    "    accuracies.append(history.history[\"val_acc\"][-1])\n",
    "    balanced_accuracies.append(bal_acc)\n",
    "    max_acc.append(np.max(history.history[\"val_acc\"]))\n",
    "    sensitivities.append(sens)\n",
    "    specificities.append(spec)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training Time: {}h:{}m:{}s\".format(\n",
    "            training_time//3600, (training_time//60)%60, training_time%60))\n",
    "\n",
    "print(\"Validation final accuracies: \\n {}\".format(accuracies))\n",
    "print(\"Validation final accuracies mean: {}\".format(np.mean(accuracies)))\n",
    "print(\"Validation best accuracies: \\n {}\".format(max_acc))\n",
    "print(\"Validation best accuracies mean: {}\".format(np.mean(max_acc)))\n",
    "print(\"Validation balanced accuracies: \\n {}\".format(balanced_accuracies))\n",
    "print(\"Validation balanced accuracies mean: {}\".format(np.mean(balanced_accuracies)))\n",
    "print(\"Validation final sensitivities: \\n {}\".format(sensitivities))\n",
    "print(\"Validation final sensitivities' mean: {}\".format(np.mean(sensitivities)))\n",
    "print(\"Validation final specificities: \\n {}\".format(specificities))\n",
    "print(\"Validation final specificities' mean: {}\".format(np.mean(specificities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [\"weights-augm-trial%i-improvement-BEST.hdf5\"%i for i in range(num_trials)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load holdout set\n",
    "test_loader = CISDataset(X_holdout, y_holdout, transform=[intensity], batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Both axis > a.ndim and axis < -a.ndim - 1 are deprecated and will raise an AxisError in the future.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy 34.78 %\n",
      "Balanced accuracy 35.38 %\n",
      "Sensitivity 30.77 %\n",
      "Specificity 40.00 %\n",
      "Fold 1\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 2\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 3\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 4\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 5\n",
      "Model accuracy 60.87 %\n",
      "Balanced accuracy 65.38 %\n",
      "Sensitivity 30.77 %\n",
      "Specificity 100.00 %\n",
      "Fold 6\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 7\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 8\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "Fold 9\n",
      "Model accuracy 43.48 %\n",
      "Balanced accuracy 50.00 %\n",
      "Sensitivity 0.00 %\n",
      "Specificity 100.00 %\n",
      "######## Final results ########\n",
      "Accuracy mean 44.35 %\n",
      "Balanced accuracy mean 50.08 %\n",
      "Sensitivity mean 6.15 %\n",
      "Specificity mean 94.00 %\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "balanced_accuracies = []\n",
    "sensitivities = []\n",
    "specificities = []\n",
    "for fold, weight in enumerate(weights):\n",
    "    print(\"Fold {}\".format(fold))\n",
    "    model = load_model(model_path)\n",
    "    model_dir = os.path.join(result_dir, weight)\n",
    "    model.load_weights(model_dir)\n",
    "    \n",
    "    opti = Adam(lr=lr, decay=lr_decay)\n",
    "    model.compile(optimizer=opti,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    # Evaluate\n",
    "    res = model.evaluate_generator(test_loader)\n",
    "    y_pred = model.predict_generator(test_loader)\n",
    "    bal_acc = balanced_accuracy(y_holdout, y_pred>0.5)\n",
    "    sens = sensitivity(y_holdout, y_pred>0.5)\n",
    "    spec = specificity(y_holdout, y_pred>0.5)\n",
    "    # Store results\n",
    "    accuracies.append(res[1])\n",
    "    balanced_accuracies.append(bal_acc)\n",
    "    sensitivities.append(sens)\n",
    "    specificities.append(spec)\n",
    "    # Print results\n",
    "    print(\"Model accuracy {:.2f} %\".format(res[1]*100))\n",
    "    print(\"Balanced accuracy {:.2f} %\".format(bal_acc*100))\n",
    "    print(\"Sensitivity {:.2f} %\".format(sens*100))\n",
    "    print(\"Specificity {:.2f} %\".format(spec*100))\n",
    "    \n",
    "    \n",
    "print(\"######## Final results ########\")\n",
    "print(\"Accuracy mean {:.2f} %\".format(np.mean(accuracies)*100))\n",
    "print(\"Balanced accuracy mean {:.2f} %\".format(np.mean(balanced_accuracies)*100))\n",
    "print(\"Sensitivity mean {:.2f} %\".format(np.mean(sensitivities)*100))\n",
    "print(\"Specificity mean {:.2f} %\".format(np.mean(specificities)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.0h:13.0m:0.6913394927978516s\n",
      "Total time elapsed: 0.0h:15.0m:4.118826389312744s\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Time: {}h:{}m:{}s\".format(\n",
    "            training_time//3600, (training_time//60)%60, training_time%60))\n",
    "print(\"Total time elapsed: {}h:{}m:{}s\".format(\n",
    "            total_time//3600, (total_time//60)%60, total_time%60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (postal)",
   "language": "python",
   "name": "postal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
